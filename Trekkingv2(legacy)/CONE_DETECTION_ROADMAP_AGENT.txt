================================================================================
ROADMAP COMPLETO E DETALHADO - SISTEMA DE DETEC√á√ÉO DE CONES
================================================================================

Data de gera√ß√£o: 2026-01-15
Gerado por: narutojgdr-sudo

================================================================================
RESUMO EXECUTIVO
================================================================================

Este documento apresenta um roadmap t√©cnico COMPLETO e EXTENSIVAMENTE DETALHADO
de TODAS as funcionalidades relacionadas √† detec√ß√£o de cones (cone detection) 
no reposit√≥rio narutojgdr-sudo/Trekking-DragonBotZv2.

O sistema implementa detec√ß√£o e rastreamento em tempo real de cones de tr√¢nsito
utilizando t√©cnicas de vis√£o computacional baseadas em segmenta√ß√£o por cor e
valida√ß√£o geom√©trica. A arquitetura √© modular, permitindo f√°cil manuten√ß√£o e
extens√£o.

OBJETIVO PRINCIPAL:
Fornecer um sistema robusto de detec√ß√£o de m√∫ltiplos cones em tempo real para
aplica√ß√µes de navega√ß√£o aut√¥noma, com capacidade de tracking, confirma√ß√£o de
detec√ß√µes e estimativa de posi√ß√£o/orienta√ß√£o.

COMPONENTES PRINCIPAIS:
- Sistema de detec√ß√£o baseado em m√∫ltiplas m√°scaras de cor (HSV, Lab, RG, backprojection)
- Tracker multi-objeto com associa√ß√£o greedy e suaviza√ß√£o EMA
- Visualizador com informa√ß√µes de debug
- Sistema de configura√ß√£o hot-reload (YAML)
- Suporte para entrada de v√≠deo e c√¢mera
- Sa√≠da de v√≠deo processado para ambientes headless


================================================================================
INVENT√ÅRIO DETALHADO - MAPA DE FUNCIONALIDADES
================================================================================

Este invent√°rio documenta TODOS os scripts e m√≥dulos relacionados √† detec√ß√£o
de cones, com descri√ß√£o completa de funcionalidades, classes, fun√ß√µes, fluxos
internos e interdepend√™ncias.

--------------------------------------------------------------------------------
1. M√ìDULO: cone_tracker/app.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/app.py

DESCRI√á√ÉO DETALHADA:
Aplica√ß√£o principal que orquestra todo o sistema de detec√ß√£o e tracking de cones.
Respons√°vel por:
- Inicializar e gerenciar captura de v√≠deo/c√¢mera
- Coordenar detector, tracker e visualizador
- Implementar loop principal de processamento
- Gerenciar configura√ß√£o com hot-reload
- Calcular e exibir informa√ß√µes de debug (heading, dist√¢ncia estimada)
- Salvar v√≠deo processado quando configurado

CLASSES E M√âTODOS PRINCIPAIS:

Classe: App
-----------
Assinatura: App()

M√©todo: __init__(self)
  - Descri√ß√£o: Inicializa a aplica√ß√£o carregando configura√ß√£o e criando componentes
  - Par√¢metros: Nenhum
  - Retorno: None
  - Comportamento: 
    * Carrega configura√ß√£o via load_config()
    * Cria inst√¢ncias de ConeDetector, MultiConeTracker e Visualizer
    * Inicializa vari√°veis de controle de reload de config

M√©todo: reload_config(self)
  - Descri√ß√£o: Recarrega configura√ß√£o do arquivo YAML e reinicializa componentes
  - Par√¢metros: Nenhum
  - Retorno: None
  - Comportamento:
    * Recarrega config via load_config()
    * Reinicializa detector, tracker e visualizer com nova config
    * Define mensagem de reload para exibir por 3 segundos
    * Registra log de sucesso

M√©todo: _debug_print_heading(self, tracks: list, frame_w: int)
  - Descri√ß√£o: Imprime informa√ß√µes de dire√ß√£o/steering para debug no terminal
  - Par√¢metros:
    * tracks: Lista de objetos Track para processar
    * frame_w: Largura do frame em pixels (para calcular centro)
  - Retorno: None
  - Comportamento:
    * Verifica se debug print_heading est√° habilitado
    * Obt√©m HFOV (campo de vis√£o horizontal) da config, default 70¬∞
    * Valida HFOV (range 10-170¬∞), usa fallback se inv√°lido
    * Calcula focal length usando modelo pinhole: focal = (width/2) / tan(hfov/2)
    * Para cada track:
      - Calcula erro horizontal em pixels (cx - center_x)
      - Converte erro para √¢ngulo: angle = arctan(err_px / focal_px)
      - Se cone_height_m configurado e bbox_h > MIN_BBOX_HEIGHT_FOR_DISTANCE:
        + Estima dist√¢ncia: dist = (cone_height * focal) / bbox_height
      - Imprime log formatado: "HEADING_DBG: detected=True id=X cx=Y err_px=Z err_deg=W bbox_h=H [est_dist=D] avg_score=S"

M√©todo: run(self)
  - Descri√ß√£o: Loop principal da aplica√ß√£o
  - Par√¢metros: Nenhum
  - Retorno: None (loop infinito at√© quit)
  - Comportamento detalhado:
    FASE 1 - Inicializa√ß√£o de captura:
      * Verifica se video_path est√° configurado
      * Se v√≠deo existe: usa cv2.VideoCapture(video_path)
      * Se v√≠deo n√£o existe: avisa e faz fallback para c√¢mera
      * Se sem v√≠deo: usa cv2.VideoCapture(cam_index, CAP_V4L2)
      * Valida se captura abriu (cap.isOpened())
      * Se usando c√¢mera: aplica configura√ß√µes (width, height, fps)
    
    FASE 2 - Setup de sa√≠da de v√≠deo:
      * Se output_video_path configurado:
        - Cria VideoWriter com codec 'mp4v'
        - Usa FPS e tamanho do processo
        - Registra log de salvamento
    
    FASE 3 - Inicializa√ß√£o de controle:
      * Inicializa contador de tempo (t_last)
      * Contador de falhas consecutivas (fail_count)
      * M√°ximo de falhas permitidas (max_fail, default 120)
      * Inicializa watcher de config
    
    FASE 4 - Loop principal (while True):
      PASSO 1: Verificar mudan√ßas em config
        - Chama watch_config(path)
        - Se mudou: chama self.reload_config()
        - Limpa mensagem de reload ap√≥s 3 segundos
      
      PASSO 2: Capturar frame
        - L√™ frame via cap.read()
        - Se falha e usando v√≠deo: reinicia do in√≠cio (POS_FRAMES=0)
        - Se falha e usando c√¢mera: incrementa fail_count, sai se > max_fail
        - Reseta fail_count em sucesso
      
      PASSO 3: Processar frame
        - Redimensiona para process_width x process_height
        - Chama detector.detect(proc) -> (detections, mask, rejects)
        - Se log_rejections habilitado: registra at√© 5 rejei√ß√µes no console
      
      PASSO 4: Atualizar tracker
        - Chama tracker.update(detections)
        - Obt√©m tracks confirmados via tracker.confirmed_tracks()
        - Chama _debug_print_heading() para imprimir info de dire√ß√£o
      
      PASSO 5: Log de suspects
        - Se log_suspects habilitado: registra at√© 5 suspects no console
      
      PASSO 6: Calcular FPS
        - Calcula fps = 1.0 / (now - t_last)
        - Atualiza t_last
      
      PASSO 7: Visualiza√ß√£o
        - Seleciona tracks para desenhar (todos se draw_suspects=true, sen√£o s√≥ confirmed)
        - Chama vis.draw() para gerar frame anotado
        - Se video_writer ativo: escreve frame
      
      PASSO 8: Exibir janelas (se habilitado)
        - Mostra janela "Tracker" com frame anotado
        - Se show_mask: mostra janela "Mask"
        - Processa teclas:
          + 'q': sai do loop
          + 's': salva config atual
          + 'r': for√ßa reload de config
        - Trata exce√ß√µes de GUI (ambientes headless): desabilita show_windows automaticamente
    
    FASE 5 - Finaliza√ß√£o (finally):
      * Libera captura (cap.release())
      * Se video_writer ativo: libera e registra log de sucesso
      * Destr√≥i janelas OpenCV (ignora erros em ambientes sem GUI)

FLUXO INTERNO (PSEUDOC√ìDIGO):
```
fun√ß√£o run():
  // Setup inicial
  se video_path configurado e arquivo existe:
    captura = VideoCapture(video_path)
    usando_video = true
  sen√£o:
    captura = VideoCapture(camera_index)
    usando_video = false
  
  se output_video_path configurado:
    video_writer = VideoWriter(output_path, codec='mp4v', fps, size)
  
  // Loop principal
  enquanto verdadeiro:
    // 1. Verificar config
    se config mudou:
      recarregar_config()
    
    // 2. Capturar
    ret, frame = captura.ler()
    se n√£o ret:
      se usando_video:
        reiniciar_video()
      sen√£o:
        incrementar_falhas()
        se falhas > max:
          sair
    
    // 3. Processar
    frame_processado = redimensionar(frame)
    detections, mask, rejects = detector.detect(frame_processado)
    
    // 4. Tracking
    tracker.update(detections)
    tracks_confirmados = tracker.confirmed_tracks()
    imprimir_debug_heading(tracks_confirmados, largura_frame)
    
    // 5. Visualizar
    tracks_desenhar = todos_tracks se draw_suspects sen√£o tracks_confirmados
    frame_saida = visualizer.draw(frame_processado, tracks_desenhar, rejects, fps)
    
    // 6. Salvar/mostrar
    se video_writer:
      video_writer.escrever(frame_saida)
    se show_windows:
      mostrar_janela(frame_saida)
      processar_teclas()
```

CONEX√ïES COM OUTROS COMPONENTES:
- IMPORTA: config.load_config, config.save_config, config.watch_config
- IMPORTA: detector.ConeDetector
- IMPORTA: tracker.MultiConeTracker
- IMPORTA: visualizer.Visualizer
- IMPORTA: utils.ConeState
- EXPORTADO VIA: cone_tracker/__init__.py (classe App)
- CHAMADO POR: test7.py (App().run())

PAR√ÇMETROS CONFIGUR√ÅVEIS (via cone_config.yaml):
- camera.video_path: Caminho para arquivo de v√≠deo (string, vazio usa c√¢mera)
- camera.output_video_path: Caminho para salvar v√≠deo processado (string, vazio n√£o salva)
- camera.index: √çndice da c√¢mera (int, default 0)
- camera.capture_width: Largura de captura (int, default 1920)
- camera.capture_height: Altura de captura (int, default 1080)
- camera.process_width: Largura de processamento (int, default 960)
- camera.process_height: Altura de processamento (int, default 540)
- camera.fps: FPS da c√¢mera (int, default 30)
- camera.max_consecutive_read_failures: M√°ximo de falhas consecutivas (int, default 120)
- camera.hfov_deg: Campo de vis√£o horizontal em graus (float, default 70.0)
- debug.show_windows: Mostrar janelas GUI (bool, default true)
- debug.show_mask: Mostrar m√°scara HSV (bool, default true)
- debug.print_heading: Imprimir info de dire√ß√£o no terminal (bool, default false)
- debug.show_heading_overlay: Quando true, o overlay visual desenha dire√ß√£o + graus acima de cada bbox CONFIRMED (bool, default false)
- debug.heading_center_deadband_deg: Deadband threshold em graus; dentro desse erro absoluto o r√≥tulo se torna CENTER (float, default 0.5)
- debug.cone_height_m: Altura real do cone em metros (float ou null, para estimar dist√¢ncia)
- debug.draw_suspects: Desenhar tracks suspects em amarelo (bool, default false)
- debug.log_rejections: Registrar rejei√ß√µes no console (bool, default false)
- debug.log_suspects: Registrar suspects no console (bool, default false)

DEPEND√äNCIAS EXTERNAS:
- cv2 (OpenCV): Captura de v√≠deo, processamento de imagem
- logging: Sistema de logs
- time: Medi√ß√£o de tempo e FPS
- math: C√°lculos trigonom√©tricos para heading
- os: Verifica√ß√£o de exist√™ncia de arquivos

DEPEND√äNCIAS INTERNAS:
- cone_tracker.config
- cone_tracker.detector
- cone_tracker.tracker
- cone_tracker.utils
- cone_tracker.visualizer

PONTOS DE ATEN√á√ÉO:
1. CONSTANTE MIN_BBOX_HEIGHT_FOR_DISTANCE (10.0): Define altura m√≠nima do bbox
   para calcular dist√¢ncia estimada. Valores muito pequenos geram estimativas
   irrealistas.
2. Valida√ß√£o de HFOV (10-170¬∞): HFOV fora desse range usa fallback de 70¬∞.
   Ajustar conforme c√¢mera real para c√°lculos precisos de √¢ngulo.
3. Tratamento de GUI em ambientes headless: O c√≥digo captura cv2.error e
   desabilita automaticamente show_windows. Ainda assim, recomenda-se
   configurar show_windows=false para ambientes sem display.
4. Loop de v√≠deo: Quando usando v√≠deo, ao chegar no fim, reinicia automaticamente
   do frame 0. √ötil para testes cont√≠nuos.
5. Hot-reload de config: Mudan√ßas no arquivo cone_config.yaml s√£o detectadas
   automaticamente e aplicadas sem reiniciar aplica√ß√£o. Por√©m, isso reinicializa
   todos os componentes, zerando hist√≥rico de tracks.

LIMITA√á√ïES:
- N√£o suporta m√∫ltiplas c√¢meras simult√¢neas
- N√£o persiste estado de tracking entre reloads de config
- Estimativa de dist√¢ncia assume modelo pinhole simples (sem corre√ß√£o de distor√ß√£o)
- C√°lculo de heading assume c√¢mera nivelada (sem pitch/roll)
- N√£o implementa grava√ß√£o de logs estruturados para an√°lise posterior

RECOMENDA√á√ïES T√âCNICAS:
1. Para ambientes de produ√ß√£o, desabilitar debug.print_heading para reduzir
   overhead de logs.
2. Configurar output_video_path para capturar resultados em ambientes headless.
3. Ajustar camera.hfov_deg conforme especifica√ß√µes reais da c√¢mera para
   c√°lculos precisos de √¢ngulo.
4. Se cone_height_m n√£o for conhecido com precis√£o, deixar como null para
   evitar estimativas de dist√¢ncia incorretas.
5. Para rastreamento de cones em movimento r√°pido, aumentar
   tracking.association_max_distance e ajustar tracking.ema_alpha.
6. Em ambientes com varia√ß√µes de ilumina√ß√£o, habilitar color.enable_gray_world.


--------------------------------------------------------------------------------
2. M√ìDULO: cone_tracker/detector.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/detector.py

DESCRI√á√ÉO DETALHADA:
Implementa a l√≥gica principal de detec√ß√£o de cones usando segmenta√ß√£o por cor
e valida√ß√£o geom√©trica. O detector usa m√∫ltiplas t√©cnicas de segmenta√ß√£o de cor
(HSV, Lab, R/G chromaticity, backprojection) combinadas via OR l√≥gico, seguido
de agrupamento de partes conectadas e valida√ß√£o geom√©trica rigorosa.

RESPONSABILIDADES:
- Pr√©-processamento de imagens (blur, CLAHE)
- Segmenta√ß√£o multi-modal de cor (4 t√©cnicas opcionais)
- Opera√ß√µes morfol√≥gicas (abertura/fechamento)
- Detec√ß√£o de contornos e extra√ß√£o de partes
- Agrupamento de partes verticalmente alinhadas
- Valida√ß√£o geom√©trica (√°rea, aspect ratio, fill ratio)
- C√°lculo de score de perfil (profile score) baseado em forma c√¥nica
- Retorno de detec√ß√µes classificadas por score

CLASSES E M√âTODOS PRINCIPAIS:

Classe: ConeDetector
--------------------
Assinatura: ConeDetector(config: Dict[str, Any])

M√©todo: __init__(self, config: Dict[str, Any])
  - Descri√ß√£o: Inicializa detector com configura√ß√£o
  - Par√¢metros:
    * config: Dicion√°rio de configura√ß√£o completo
  - Retorno: None
  - Comportamento:
    * Extrai thresholds HSV para laranja (2 ranges para wraparound em Hue)
    * Cria kernels de morfologia (abertura e fechamento)
    * Cria objeto CLAHE para equaliza√ß√£o adaptativa de histograma
    * Carrega op√ß√µes de cor (gray_world, lab, rg, backprojection)
    * Se backprojection habilitado: carrega histograma de arquivo .npy
  - Configura√ß√µes carregadas:
    + hsv_orange.low_1, high_1, low_2, high_2: Arrays numpy [H,S,V]
    + morphology.kernel_open, kernel_close: Tamanho dos kernels
    + clahe.clip_limit: Limite de recorte (float, default 1.8)
    + clahe.tile_grid_size: Tamanho da grade de tiles ([int, int])
    + color.enable_gray_world: Habilitar normaliza√ß√£o gray-world (bool)
    + color.enable_lab_fallback: Habilitar m√°scara Lab a/b (bool)
    + color.lab_a_range, lab_b_range: Ranges para canais Lab a e b
    + color.enable_rg: Habilitar m√°scara r/g chromaticity (bool)
    + color.rg_thresholds: Dicion√°rio com r_min, r_max, g_min, g_max
    + color.enable_backproj: Habilitar backprojection (bool)
    + color.backproj_hist_path: Caminho para arquivo .npy do histograma
    + color.backproj_thresh: Threshold de backprojection (int, default 50)

M√©todo: preprocess(self, bgr: np.ndarray) -> np.ndarray
  - Descri√ß√£o: Pr√©-processa imagem BGR para HSV com normaliza√ß√µes opcionais
  - Par√¢metros:
    * bgr: Imagem BGR (numpy array HxWx3, uint8)
  - Retorno: Imagem HSV pr√©-processada (numpy array HxWx3, uint8)
  - Fluxo interno:
    1. Se enable_gray_world: aplica gray_world(bgr) para normaliza√ß√£o de cor
    2. Aplica GaussianBlur(5x5) para reduzir ru√≠do
    3. Converte BGR para HSV
    4. Separa canais H, S, V
    5. Aplica CLAHE somente no canal V (brilho)
    6. Recomp√µe e retorna imagem HSV

M√©todo: get_mask(self, bgr: np.ndarray, hsv: np.ndarray) -> np.ndarray
  - Descri√ß√£o: Combina m√∫ltiplas m√°scaras de cor via OR l√≥gico
  - Par√¢metros:
    * bgr: Imagem BGR original (para Lab, RG, backproj)
    * hsv: Imagem HSV pr√©-processada
  - Retorno: M√°scara bin√°ria combinada (uint8, 0 ou 255)
  - Fluxo interno:
    1. M√ÅSCARA HSV (sempre ativa):
       - m1 = inRange(hsv, low_1, high_1)
       - m2 = inRange(hsv, low_2, high_2)
       - mask_hsv = OR(m1, m2)
       - Adiciona mask_hsv √† lista de m√°scaras
    
    2. M√ÅSCARA LAB (se enable_lab_fallback):
       - Converte BGR para Lab
       - Extrai canais a e b
       - mask_a = inRange(a, lab_a_range)
       - mask_b = inRange(b, lab_b_range)
       - mask_lab = AND(mask_a, mask_b)
       - Adiciona mask_lab √† lista
    
    3. M√ÅSCARA R/G CHROMATICITY (se enable_rg):
       - Chama rg_chromaticity_mask(bgr, rg_thresholds)
       - Adiciona √† lista
    
    4. M√ÅSCARA BACKPROJECTION (se enable_backproj e hist dispon√≠vel):
       - Chama mask_from_backproj(hsv, hist, thresh)
       - Adiciona √† lista
    
    5. COMBINA√á√ÉO:
       - combined = masks[0]
       - Para cada m√°scara adicional: combined = OR(combined, m√°scara)
    
    6. MORFOLOGIA:
       - Aplica morphologyEx(MORPH_OPEN) com k_open, open_iterations
       - Aplica morphologyEx(MORPH_CLOSE) com k_close, close_iterations
    
    7. Retorna m√°scara combinada e limpa

M√©todo: _part_boxes(self, mask: np.ndarray) -> List[Tuple[int, int, int, int]]
  - Descri√ß√£o: Encontra bounding boxes de partes individuais na m√°scara
  - Par√¢metros:
    * mask: M√°scara bin√°ria (uint8)
  - Retorno: Lista de bounding boxes (x, y, w, h) ordenados por y crescente
  - Fluxo interno:
    1. Encontra contornos via findContours(RETR_EXTERNAL, CHAIN_APPROX_SIMPLE)
    2. Para cada contorno:
       - Calcula √°rea via contourArea()
       - Se √°rea < min_part_area: descarta
       - Sen√£o: adiciona boundingRect() √† lista
    3. Ordena boxes por coordenada y (top to bottom)
    4. Retorna lista ordenada

M√©todo: _build_groups(self, boxes: List[Tuple[int, int, int, int]]) -> List[List[int]]
  - Descri√ß√£o: Agrupa partes verticalmente alinhadas usando grafo de conectividade
  - Par√¢metros:
    * boxes: Lista de bounding boxes de partes
  - Retorno: Lista de grupos (cada grupo √© lista de √≠ndices de boxes)
  - Fluxo interno (ALGORITMO DE CONECTIVIDADE):
    1. Inicializa lista de adjac√™ncia adj[n] (n = n√∫mero de boxes)
    2. Para cada par de boxes (i, j) onde j > i:
       a. Calcula centros horizontais acx, bcx
       b. Calcula gap vertical: gap = by - (ay + ah)
       c. Se gap > max_y_gap: continua (muito distante verticalmente)
       d. Calcula sobreposi√ß√£o horizontal: ov = x_overlap_ratio(box_i, box_j)
       e. Verifica se centros est√£o pr√≥ximos: |acx - bcx| <= max_x_center_diff
       f. Se ov >= min_x_overlap_ratio OU centros pr√≥ximos:
          - Adiciona j a adj[i] e i a adj[j] (conex√£o bidirecional)
    3. Executa DFS (busca em profundidade) para encontrar componentes conectados:
       a. visited[n] = [False, ...]
       b. Para cada n√≥ i n√£o visitado:
          - Inicia DFS com pilha [i]
          - Marca visitado e expande vizinhos
          - Coleta todos n√≥s alcan√ß√°veis em comp[]
          - Adiciona comp a groups[]
    4. Retorna lista de grupos
  - Par√¢metros usados:
    + grouping.max_y_gap: Gap vertical m√°ximo entre partes (pixels)
    + grouping.min_x_overlap_ratio: Sobreposi√ß√£o horizontal m√≠nima (0-1)
    + grouping.max_x_center_diff: Diferen√ßa m√°xima entre centros (pixels)

M√©todo: _pad_bbox(self, bbox: Tuple[int, int, int, int], frame_w: int, frame_h: int) -> Tuple[int, int, int, int]
  - Descri√ß√£o: Adiciona padding ao bounding box com clipping
  - Par√¢metros:
    * bbox: Bounding box original (x, y, w, h)
    * frame_w, frame_h: Dimens√µes do frame para clipping
  - Retorno: Bounding box expandido (x, y, w, h)
  - Comportamento:
    * x1 = clamp(x - pad_x, 0, frame_w-1)
    * y1 = clamp(y - pad_y, 0, frame_h-1)
    * x2 = clamp(x + w + pad_x, 0, frame_w)
    * y2 = clamp(y + h + pad_y, 0, frame_h)
    * Retorna (x1, y1, x2-x1, y2-y1)

M√©todo: _profile_score(self, mask: np.ndarray, bbox: Tuple[int, int, int, int]) -> float
  - Descri√ß√£o: Calcula score de perfil baseado na forma c√¥nica (base larga, topo estreito)
  - Par√¢metros:
    * mask: M√°scara bin√°ria completa
    * bbox: Bounding box para analisar
  - Retorno: Score entre 0.0 e 1.0 (maior = mais c√¥nico)
  - Algoritmo detalhado:
    1. Extrai ROI segura da m√°scara
    2. Divide ROI em N slices horizontais (N = profile_slices, default 10)
    3. Para cada slice:
       a. Calcula quais colunas t√™m pelo menos 1 pixel branco (occupied width)
       b. Normaliza pelo width total: width_ratio = occupied_cols / total_cols
       c. Armazena width_ratio (valor entre 0 e 1)
    4. Filtra slices muito pequenos (width < 0.03)
    5. Se menos de 4 slices v√°lidos: retorna 0.0
    6. Calcula MONOTONICITY SCORE:
       - Percorre slices consecutivos
       - Se slice[i+1] >= slice[i] * 0.85: adiciona 1.0
       - Se slice[i+1] >= slice[i] * 0.70: adiciona 0.5
       - mono_score = soma / (n_slices - 1)
    7. Calcula TOP vs BOTTOM score:
       - top_third = m√©dia do primeiro ter√ßo dos slices
       - bot_third = m√©dia do √∫ltimo ter√ßo dos slices
       - ratio = bot_third / top_third (maior = mais largo embaixo)
       - base_score = clamp((ratio - 1.0) / 1.2, 0, 1)
    8. Combina scores: score = 0.55 * mono_score + 0.45 * base_score
    9. Retorna score final
  - Heur√≠stica: Cones t√™m base larga e topo estreito, com largura crescente
    monotonicamente de cima para baixo. O score penaliza formas irregulares.

M√©todo: detect(self, frame: np.ndarray) -> Tuple[List[Tuple[Tuple[int, int, int, int], float, dict]], np.ndarray, List[Tuple[Tuple[int, int, int, int], str]]]
  - Descri√ß√£o: Detecta cones no frame completo
  - Par√¢metros:
    * frame: Imagem BGR (numpy array HxWx3, uint8)
  - Retorno: Tupla (results, mask, rejects) onde:
    * results: Lista de detec√ß√µes aceitas (bbox, score, data_dict)
    * mask: M√°scara bin√°ria final
    * rejects: Lista de candidatos rejeitados (bbox, reason_string)
  - Fluxo completo (PIPELINE DE DETEC√á√ÉO):
    
    FASE 1 - PR√â-PROCESSAMENTO:
      1. hsv = self.preprocess(frame)
      2. mask = self.get_mask(frame, hsv)
    
    FASE 2 - EXTRA√á√ÉO DE PARTES:
      3. boxes = self._part_boxes(mask)
      4. groups_idx = self._build_groups(boxes)
    
    FASE 3 - VALIDA√á√ÉO E SCORING:
      Para cada grupo de partes conectadas:
        3.1. Union bounding box:
          - ub = bbox_union([boxes[i] for i in group])
          - group_bbox = _pad_bbox(ub, frame_w, frame_h)
        
        3.2. Valida√ß√£o de √ÅREA:
          - area = w * h
          - Se area < min_group_area OU area > max_group_area:
            + Rejeita com motivo "area=X"
            + Continua pr√≥ximo grupo
        
        3.3. Valida√ß√£o de ASPECT RATIO:
          - aspect = h / w
          - Se aspect < aspect_min OU aspect > aspect_max:
            + Rejeita com motivo "aspect=X"
            + Continua
        
        3.4. Valida√ß√£o de FILL RATIO:
          - roi = extrai ROI da m√°scara
          - fill = countNonZero(roi) / area
          - Se fill < min_fill_ratio OU fill > max_fill_ratio:
            + Rejeita com motivo "fill=X"
            + Continua
        
        3.5. C√°lculo de SCORES:
          - p = _profile_score(mask, group_bbox)
          - aspect_score = 1.0 - |aspect - 2.0| / 2.5
          - aspect_score = clamp(aspect_score, 0, 1)
          - score = (p * weight_profile) + (fill * weight_fill) + (aspect_score * weight_aspect)
        
        3.6. Valida√ß√£o de SCORE FINAL:
          - Se score < min_frame_score:
            + Rejeita com motivo "score=X"
            + Continua
        
        3.7. ACEITA DETEC√á√ÉO:
          - Cria data dict: {profile, fill, aspect, aspect_score, score, parts}
          - Adiciona (group_bbox, score, data) a results
    
    FASE 4 - ORDENA√á√ÉO E RETORNO:
      4. Ordena results por score (maior primeiro)
      5. Retorna (results, mask, rejects)

FLUXO INTERNO (PSEUDOC√ìDIGO COMPLETO):
```
fun√ß√£o detect(frame):
  // Pr√©-processamento
  hsv = preprocessar(frame)  // blur + CLAHE + convers√£o
  mask = combinar_mascaras(frame, hsv)  // HSV + Lab + RG + backproj
  
  // Extra√ß√£o de partes
  boxes = encontrar_contornos(mask)
  filtrar_por_area_minima(boxes)
  ordenar_por_y(boxes)
  
  // Agrupamento
  grupos = construir_grafo_conectividade(boxes)
  executar_DFS_para_componentes_conectados(grupos)
  
  // Valida√ß√£o e scoring
  results = []
  rejects = []
  
  para cada grupo em grupos:
    bbox = unir_boxes_do_grupo(grupo)
    bbox = adicionar_padding(bbox)
    area = bbox.w * bbox.h
    
    // Filtro 1: √Årea
    se area < min OU area > max:
      rejects.adicionar(bbox, "area=X")
      continuar
    
    // Filtro 2: Aspect ratio
    aspect = bbox.h / bbox.w
    se aspect < min OU aspect > max:
      rejects.adicionar(bbox, "aspect=X")
      continuar
    
    // Filtro 3: Fill ratio
    roi = extrair_roi(mask, bbox)
    fill = contar_pixels_brancos(roi) / area
    se fill < min OU fill > max:
      rejects.adicionar(bbox, "fill=X")
      continuar
    
    // Calcular scores
    profile = calcular_profile_score(mask, bbox)
    aspect_score = 1.0 - |aspect - 2.0| / 2.5
    score = profile*w1 + fill*w2 + aspect_score*w3
    
    // Filtro 4: Score final
    se score < threshold:
      rejects.adicionar(bbox, "score=X")
      continuar
    
    // Aceitar
    data = {profile, fill, aspect, aspect_score, score, parts}
    results.adicionar(bbox, score, data)
  
  ordenar_por_score_decrescente(results)
  retornar results, mask, rejects
```

CONEX√ïES COM OUTROS COMPONENTES:
- IMPORTA: color_utils.gray_world, load_backproj_hist, mask_from_backproj, rg_chromaticity_mask
- IMPORTA: utils.bbox_union, clamp, safe_roi, x_overlap_ratio
- USADO POR: app.App (self.detector.detect())
- EXPORTADO VIA: cone_tracker/__init__.py

PAR√ÇMETROS CONFIGUR√ÅVEIS:
- hsv_orange.low_1, high_1: Primeiro range HSV para laranja ([H,S,V])
- hsv_orange.low_2, high_2: Segundo range HSV (wraparound em Hue)
- morphology.kernel_open: Tamanho do kernel de abertura (int, default 3)
- morphology.kernel_close: Tamanho do kernel de fechamento (int, default 7)
- morphology.open_iterations: Itera√ß√µes de abertura (int, default 1)
- morphology.close_iterations: Itera√ß√µes de fechamento (int, default 1)
- clahe.clip_limit: Limite de recorte CLAHE (float, default 1.8)
- clahe.tile_grid_size: Grade de tiles CLAHE ([int, int], default [8, 8])
- grouping.min_part_area: √Årea m√≠nima de parte (int, default 80 pixels)
- grouping.max_y_gap: Gap vertical m√°ximo (int, default 80 pixels)
- grouping.min_x_overlap_ratio: Sobreposi√ß√£o horizontal m√≠nima (float, default 0.20)
- grouping.max_x_center_diff: Diferen√ßa m√°xima de centros (int, default 80 pixels)
- grouping.pad_x: Padding horizontal (int, default 8 pixels)
- grouping.pad_y: Padding vertical (int, default 12 pixels)
- geometry.min_group_area: √Årea m√≠nima de grupo (int, default 1400 pixels)
- geometry.max_group_area: √Årea m√°xima de grupo (int, default 450000 pixels)
- geometry.aspect_min: Aspect ratio m√≠nimo (float, default 1.0)
- geometry.aspect_max: Aspect ratio m√°ximo (float, default 6.0)
- geometry.profile_slices: N√∫mero de slices para profile score (int, default 10)
- geometry.min_fill_ratio: Fill ratio m√≠nimo (float, default 0.08)
- geometry.max_fill_ratio: Fill ratio m√°ximo (float, default 0.65)
- geometry.min_frame_score: Score m√≠nimo para aceita√ß√£o (float, default 0.35)
- weights.profile: Peso do profile score (float, default 0.50)
- weights.fill: Peso do fill ratio (float, default 0.35)
- weights.aspect: Peso do aspect score (float, default 0.15)
- color.*: M√∫ltiplas op√ß√µes de cor (ver se√ß√£o color_utils)
- debug.show_rejection_reason: Incluir rejects no retorno (bool)

DEPEND√äNCIAS EXTERNAS:
- cv2: findContours, boundingRect, contourArea, inRange, morphologyEx, etc.
- numpy: Arrays, opera√ß√µes matem√°ticas

DEPEND√äNCIAS INTERNAS:
- cone_tracker.color_utils
- cone_tracker.utils

PONTOS DE ATEN√á√ÉO:
1. M√öLTIPLAS M√ÅSCARAS: A combina√ß√£o via OR permite maior robustez, mas pode
   gerar falsos positivos. Ajustar thresholds individualmente.
2. PROFILE SCORE: Heur√≠stica sens√≠vel ao n√∫mero de slices. Com menos de 4 slices
   v√°lidos, retorna 0.0. Aumentar profile_slices para objetos grandes.
3. ASPECT RATIO TARGET: O c√≥digo assume aspect ratio ideal de 2.0 (altura = 2x largura).
   Ajustar se cones tiverem propor√ß√µes diferentes.
4. PADDING: Adiciona pixels extras ao bbox para capturar bordas. Pode causar
   sobreposi√ß√£o de detec√ß√µes pr√≥ximas.
5. ORDENA√á√ÉO POR SCORE: Results s√£o ordenados por score decrescente. O tracker
   associa em ordem, priorizando detec√ß√µes de maior confian√ßa.

LIMITA√á√ïES:
- N√£o suporta detec√ß√£o de cones parcialmente oclu√≠dos (algoritmo de partes
  conectadas falha em oclus√µes severas)
- Assume cones em p√© (vertical). Cones deitados ou inclinados n√£o s√£o detectados.
- Sens√≠vel a condi√ß√µes de ilumina√ß√£o (mesmo com CLAHE e gray-world)
- N√£o usa aprendizado de m√°quina (CNN), depende de regras handcrafted
- Profile score assume crescimento monot√¥nico de largura, o que pode falhar
  para cones com listras ou reflexos

RECOMENDA√á√ïES T√âCNICAS:
1. CALIBRA√á√ÉO DE COR: Executar em v√≠deos de teste e ajustar ranges HSV/Lab
   para diferentes condi√ß√µes de ilumina√ß√£o.
2. BACKPROJECTION: Para maior robustez, gerar histograma de cones conhecidos
   e habilitar color.enable_backproj.
3. TUNING DE GEOMETRIA: Coletar estat√≠sticas de detec√ß√µes corretas (√°rea,
   aspect, fill) e ajustar ranges geometry.*.
4. PROFILE SLICES: Aumentar para 15-20 se cones forem grandes na imagem (perto).
5. FALSOS POSITIVOS: Se muitos, aumentar min_frame_score ou confirm_avg_score.
6. FALSOS NEGATIVOS: Se muitos, diminuir thresholds ou habilitar m√°scaras adicionais.
7. PERFORMANCE: Desabilitar m√°scaras n√£o essenciais (Lab, RG, backproj) se CPU limitada.
8. VALIDA√á√ÉO: Usar debug.show_rejection_reason para entender quais filtros
   est√£o rejeitando detec√ß√µes verdadeiras.


--------------------------------------------------------------------------------
3. M√ìDULO: cone_tracker/tracker.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/tracker.py

DESCRI√á√ÉO DETALHADA:
Implementa sistema de tracking multi-objeto para rastreamento de m√∫ltiplos cones
simultaneamente. Usa algoritmo de associa√ß√£o greedy baseado em dist√¢ncia euclidiana
entre centros de bounding boxes, com suaviza√ß√£o via m√©dia m√≥vel exponencial (EMA)
e gerenciamento de estados (SUSPECT ‚Üí CONFIRMED ‚Üí DELETED).

RESPONSABILIDADES:
- Manter hist√≥rico de tracks (objetos rastreados)
- Associar detec√ß√µes novas a tracks existentes
- Criar novos tracks para detec√ß√µes n√£o associadas
- Atualizar posi√ß√£o e score de tracks via EMA
- Gerenciar estados do ciclo de vida (SUSPECT, CONFIRMED, LOST)
- Deletar tracks antigos ou perdidos
- Implementar grace period para tracks confirmados

CLASSES E M√âTODOS PRINCIPAIS:

Dataclass: Track
----------------
Assinatura: Track(track_id: int)

Atributos:
  - track_id: int - Identificador √∫nico do track
  - cx, cy: float - Centro do bounding box (x, y)
  - w, h: float - Largura e altura do bounding box
  - state: ConeState - Estado atual (SUSPECT, CONFIRMED, LOST)
  - created_at: float - Timestamp de cria√ß√£o (time.time())
  - last_seen: float - Timestamp da √∫ltima detec√ß√£o associada
  - last_good_time: float - Timestamp da √∫ltima vez em estado CONFIRMED
  - score_hist: deque - Hist√≥rico de scores (maxlen configur√°vel)
  - miss_counter: int - Contador de frames consecutivos sem associa√ß√£o

M√©todo: bbox(self) -> Tuple[int, int, int, int]
  - Descri√ß√£o: Retorna bounding box como tupla (x, y, w, h)
  - Retorno: (int(cx - w/2), int(cy - h/2), int(w), int(h))

Propriedade: frames_seen(self) -> int
  - Descri√ß√£o: N√∫mero de frames em que o track foi visto (tamanho do hist√≥rico)
  - Retorno: len(score_hist)

M√©todo: avg_score(self) -> float
  - Descri√ß√£o: Calcula score m√©dio do hist√≥rico
  - Retorno: mean(score_hist) ou 0.0 se vazio

M√©todo: update(self, bbox: Tuple[int, int, int, int], score: float, alpha: float, score_window: int)
  - Descri√ß√£o: Atualiza track com nova detec√ß√£o
  - Par√¢metros:
    * bbox: Bounding box da detec√ß√£o (x, y, w, h)
    * score: Score da detec√ß√£o
    * alpha: Fator de suaviza√ß√£o EMA (0-1, maior = mais reativo)
    * score_window: Tamanho m√°ximo do hist√≥rico
  - Comportamento:
    1. Ajusta maxlen do score_hist se mudou
    2. Calcula centro da nova detec√ß√£o (cx_new, cy_new)
    3. Se primeira atualiza√ß√£o (w <= 0): inicializa diretamente
    4. Sen√£o: aplica EMA:
       - cx = alpha * cx_new + (1-alpha) * cx_old
       - cy = alpha * cy_new + (1-alpha) * cy_old
       - w = alpha * w_new + (1-alpha) * w_old
       - h = alpha * h_new + (1-alpha) * h_old
    5. Adiciona score ao hist√≥rico
    6. Atualiza timestamps (last_seen, last_good_time)
    7. Reseta miss_counter = 0

Classe: MultiConeTracker
-------------------------
Assinatura: MultiConeTracker(config: Dict[str, Any])

M√©todo: __init__(self, config: Dict[str, Any])
  - Descri√ß√£o: Inicializa tracker com configura√ß√£o
  - Par√¢metros:
    * config: Dicion√°rio de configura√ß√£o completo
  - Comportamento:
    * Extrai se√ß√µes tracking e geometry da config
    * Inicializa tracks = [] (lista vazia de tracks)
    * Inicializa next_id = 0 (contador de IDs)

M√©todo: _make_track(self, det_bbox: Tuple[int, int, int, int], det_score: float) -> Track
  - Descri√ß√£o: Cria novo track a partir de uma detec√ß√£o
  - Par√¢metros:
    * det_bbox: Bounding box da detec√ß√£o
    * det_score: Score da detec√ß√£o
  - Retorno: Novo objeto Track inicializado
  - Comportamento:
    1. Cria Track com track_id = next_id
    2. Incrementa next_id
    3. Inicializa score_hist com maxlen = score_window
    4. Chama track.update() com a detec√ß√£o
    5. Retorna track

M√©todo: _associate_greedy(self, detections: List[Tuple[Tuple[int, int, int, int], float, dict]]) -> Tuple[Dict[int, int], List[int], List[int]]
  - Descri√ß√£o: Associa tracks a detec√ß√µes usando algoritmo greedy baseado em dist√¢ncia
  - Par√¢metros:
    * detections: Lista de detec√ß√µes (bbox, score, data)
  - Retorno: Tupla (matches, unmatched_tracks, unmatched_detections) onde:
    * matches: Dict {track_index: detection_index}
    * unmatched_tracks: Lista de √≠ndices de tracks n√£o associados
    * unmatched_detections: Lista de √≠ndices de detec√ß√µes n√£o associadas
  
  - Algoritmo detalhado (GREEDY ASSOCIATION):
    PASSO 1: Casos especiais
      - Se tracks vazio: retorna ({}, [], [0,1,2,...,len(detections)-1])
      - Se detections vazio: retorna ({}, [0,1,2,...,len(tracks)-1], [])
    
    PASSO 2: Calcular todas as dist√¢ncias v√°lidas
      - max_dist = association_max_distance (da config)
      - pairs = []
      - Para cada track t em tracks:
        + tb = t.bbox()
        + Para cada detection d em detections:
          * db = d[0] (bbox)
          * dist = bbox_distance(tb, db)  // dist√¢ncia euclidiana entre centros
          * Se dist <= max_dist:
            - Adiciona (dist, track_index, detection_index) a pairs
    
    PASSO 3: Ordenar pares por dist√¢ncia crescente
      - pairs.sort(key=lambda x: x[0])  // menor dist√¢ncia primeiro
    
    PASSO 4: Associa√ß√£o greedy
      - matched_tracks = set()
      - matched_dets = set()
      - matches = {}
      - Para cada (dist, ti, di) em pairs:
        + Se ti j√° est√° em matched_tracks OU di j√° est√° em matched_dets:
          * Continua (j√° associado)
        + Sen√£o:
          * matches[ti] = di
          * Adiciona ti a matched_tracks
          * Adiciona di a matched_dets
    
    PASSO 5: Coletar n√£o associados
      - unmatched_tracks = [i for i em range(len(tracks)) if i n√£o em matched_tracks]
      - unmatched_detections = [i for i em range(len(detections)) if i n√£o em matched_dets]
    
    PASSO 6: Retornar resultados
      - return (matches, unmatched_tracks, unmatched_detections)

M√©todo: update(self, detections: List[Tuple[Tuple[int, int, int, int], float, dict]])
  - Descri√ß√£o: Atualiza tracker com novas detec√ß√µes do frame atual
  - Par√¢metros:
    * detections: Lista de detec√ß√µes do detector (bbox, score, data)
  - Retorno: None (modifica self.tracks in-place)
  
  - Fluxo completo (PIPELINE DE TRACKING):
    
    FASE 1 - EXPIRA√á√ÉO DE TRACKS ANTIGOS:
      - now = time.time()
      - alive = []
      - Para cada track t em self.tracks:
        + timeout = now - t.last_seen
        + Se timeout <= lost_timeout:
          * Adiciona t a alive
        + Sen√£o:
          * Registra log: "üóëÔ∏è Track {id} DELETADO: frames={n}, avg={score}, idade={age}s"
      - self.tracks = alive
    
    FASE 2 - ASSOCIA√á√ÉO:
      - matches, unmatched_tracks, unmatched_dets = self._associate_greedy(detections)
    
    FASE 3 - ATUALIZAR TRACKS ASSOCIADOS:
      - Para cada (ti, di) em matches.items():
        + bbox, score, _data = detections[di]
        + self.tracks[ti].update(bbox, score, alpha=ema_alpha, score_window=score_window)
    
    FASE 4 - GRACE PERIOD PARA TRACKS N√ÉO ASSOCIADOS:
      - Para cada ti em unmatched_tracks:
        + t = self.tracks[ti]
        + Se t.state == CONFIRMED:
          * t.miss_counter += 1
          * Se grace_seconds > 0.0:  // modo baseado em tempo
            - Se (now - t.last_good_time) > grace_seconds:
              + t.state = SUSPECT
              + t.score_hist.clear()
              + t.miss_counter = 0
          * Sen√£o:  // modo baseado em frames
            - grace_frames = config grace_frames
            - Se t.miss_counter > grace_frames:
              + t.state = SUSPECT
              + t.score_hist.clear()
              + t.miss_counter = 0
    
    FASE 5 - CRIAR NOVOS TRACKS:
      - Para cada di em unmatched_dets:
        + Se len(self.tracks) >= max_tracks:
          * break (limite de tracks atingido)
        + bbox, score, _data = detections[di]
        + novo_track = self._make_track(bbox, score)
        + self.tracks.append(novo_track)
    
    FASE 6 - CONFIRMAR TRACKS:
      - Para cada t em self.tracks:
        + frames = len(t.score_hist)
        + avg = t.avg_score()
        + min_frames = min_frames_for_confirm (da config)
        + threshold = confirm_avg_score (da config geometry)
        + Se frames >= min_frames E avg >= threshold:
          * Se t.state != CONFIRMED:
            - Registra log: "‚úÖ Track {id} CONFIRMADO! frames={n}, avg={score}"
          * t.state = CONFIRMED
        + Sen√£o:
          * Se t.state != CONFIRMED:
            - t.state = SUSPECT

M√©todo: confirmed_tracks(self) -> List[Track]
  - Descri√ß√£o: Retorna lista de tracks confirmados
  - Retorno: [t for t em self.tracks if t.state == CONFIRMED]
  - Nota: N√£o filtra por idade m√≠nima (min_confirmed_age_frames n√£o implementado)

FLUXO INTERNO (PSEUDOC√ìDIGO):
```
fun√ß√£o update(detections):
  // 1. Expirar tracks antigos
  agora = tempo_atual()
  tracks_vivos = []
  para cada track em self.tracks:
    se (agora - track.last_seen) <= timeout:
      tracks_vivos.adicionar(track)
    sen√£o:
      logar_deletado(track)
  self.tracks = tracks_vivos
  
  // 2. Associar tracks->detections
  matches, unmatched_t, unmatched_d = associar_greedy(detections)
  
  // 3. Atualizar tracks associados
  para cada (indice_track, indice_detection) em matches:
    bbox, score = detections[indice_detection]
    tracks[indice_track].atualizar(bbox, score, alpha, window)
  
  // 4. Grace period para tracks n√£o associados
  para cada indice_track em unmatched_t:
    track = tracks[indice_track]
    se track.state == CONFIRMED:
      track.miss_counter += 1
      se ultrapassou_grace_period(track):
        track.state = SUSPECT
        track.score_hist.limpar()
        track.miss_counter = 0
  
  // 5. Criar novos tracks
  para cada indice_detection em unmatched_d:
    se numero_de_tracks < max_tracks:
      bbox, score = detections[indice_detection]
      novo_track = criar_track(bbox, score, proximo_id)
      tracks.adicionar(novo_track)
      proximo_id += 1
  
  // 6. Decidir confirma√ß√£o
  para cada track em tracks:
    frames_vistos = tamanho_do_historico(track)
    score_medio = calcular_media(track.score_hist)
    
    se frames_vistos >= min_frames E score_medio >= threshold:
      se track.state != CONFIRMED:
        logar_confirmado(track)
      track.state = CONFIRMED
    sen√£o:
      se track.state != CONFIRMED:
        track.state = SUSPECT
```

CONEX√ïES COM OUTROS COMPONENTES:
- IMPORTA: utils.ConeState, utils.bbox_distance
- USADO POR: app.App (self.tracker.update(), self.tracker.confirmed_tracks())
- EXPORTADO VIA: cone_tracker/__init__.py

PAR√ÇMETROS CONFIGUR√ÅVEIS:
- tracking.max_tracks: M√°ximo de tracks simult√¢neos (int, default 8)
- tracking.association_max_distance: Dist√¢ncia m√°xima para associa√ß√£o (int, default 140 pixels)
- tracking.ema_alpha: Fator de suaviza√ß√£o EMA (float, 0-1, default 0.25)
- tracking.lost_timeout: Timeout em segundos para deletar track (float, default 0.6s)
- tracking.score_window: Tamanho do hist√≥rico de scores (int, default 10)
- tracking.min_frames_for_confirm: Frames m√≠nimos para confirmar (int, default 6)
- tracking.grace_frames: Frames de grace period (int, default 12)
- tracking.grace_seconds: Grace period em segundos (float, default 0.0, desabilitado)
- tracking.min_confirmed_age_frames: Idade m√≠nima para exibir (int, n√£o implementado)
- geometry.confirm_avg_score: Score m√©dio m√≠nimo para confirma√ß√£o (float, default 0.55)

DEPEND√äNCIAS EXTERNAS:
- time: Timestamps
- collections.deque: Hist√≥rico de scores
- dataclasses: Decorator @dataclass para Track
- numpy: C√°lculo de m√©dia

DEPEND√äNCIAS INTERNAS:
- cone_tracker.utils

PONTOS DE ATEN√á√ÉO:
1. ALGORITMO GREEDY: A associa√ß√£o √© greedy (primeira op√ß√£o dispon√≠vel). N√£o
   considera combina√ß√µes √≥timas globais. Para cen√°rios complexos (muitos cones
   pr√≥ximos), considerar Hungarian algorithm.
2. LOST_TIMEOUT CR√çTICO: Valor muito baixo (ex: 0.6s) causa dele√ß√£o prematura
   de tracks, impedindo confirma√ß√£o. Recomenda-se 3.0s ou mais.
3. GRACE PERIOD DUPLO: Suporta grace_seconds (tempo real) ou grace_frames (frames).
   Se grace_seconds > 0, usa tempo; sen√£o usa frames. Preferir grace_seconds
   para robustez independente de FPS.
4. RESET DE HIST√ìRICO: Quando track CONFIRMED volta a SUSPECT, o score_hist
   √© limpo. Isso for√ßa reacumula√ß√£o de evid√™ncias.
5. NEXT_ID CRESCENTE: IDs nunca s√£o reutilizados. Em execu√ß√µes longas, IDs
   podem ficar muito grandes (n√£o √© problema pr√°tico).
6. SUAVIZA√á√ÉO EMA: Alpha=0.25 significa 25% de peso para nova detec√ß√£o, 75%
   para valor anterior. Aumentar alpha para rastreamento mais reativo.

LIMITA√á√ïES:
- N√£o suporta predi√ß√£o de movimento (Kalman filter, etc.)
- Associa√ß√£o greedy n√£o √© √≥tima para cen√°rios complexos
- N√£o trata oclus√µes explicitamente
- N√£o persiste tracks entre execu√ß√µes (reinicializa√ß√£o perde hist√≥rico)
- min_confirmed_age_frames declarado na config mas n√£o implementado no c√≥digo
- N√£o suporta merge de tracks (mesmo cone detectado como 2 tracks separados)
- N√£o suporta split de tracks (oclus√£o tempor√°ria cria novo track)

RECOMENDA√á√ïES T√âCNICAS:
1. TUNING DE TIMEOUT: Configurar lost_timeout >= 3.0s para permitir confirma√ß√£o
   mesmo com detec√ß√µes intermitentes.
2. ASSOCIATION DISTANCE: Aumentar association_max_distance (250-300) para cones
   em movimento r√°pido ou c√¢mera em movimento.
3. CONFIRMA√á√ÉO R√ÅPIDA: Diminuir min_frames_for_confirm (4 frames) e
   confirm_avg_score (0.50) para confirma√ß√£o mais r√°pida, aceitando mais falsos
   positivos.
4. EMA ALPHA: Aumentar para 0.35-0.40 se cones se moverem rapidamente.
   Diminuir para 0.15-0.20 para suaviza√ß√£o mais agressiva.
5. MAX TRACKS: Ajustar conforme cen√°rio. 8 √© adequado para pistas com poucos cones.
   Aumentar para 15-20 em ambientes densos.
6. GRACE PERIOD: Usar grace_seconds em vez de grace_frames para robustez
   independente de FPS vari√°vel.
7. LOG DE DEBUG: Habilitar tracking logs para diagnosticar problemas de
   associa√ß√£o e confirma√ß√£o.
8. IMPLEMENTAR PREDI√á√ÉO: Para robustez em oclus√µes, implementar Kalman filter
   ou modelo de velocidade constante para predizer posi√ß√µes.
9. HUNGARIAN ALGORITHM: Para associa√ß√£o √≥tima em cen√°rios com muitos cones
   pr√≥ximos, substituir greedy por Hungarian (scipy.optimize.linear_sum_assignment).


--------------------------------------------------------------------------------
4. M√ìDULO: cone_tracker/utils.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/utils.py

DESCRI√á√ÉO DETALHADA:
M√≥dulo de fun√ß√µes utilit√°rias gerais para opera√ß√µes com bounding boxes,
extra√ß√£o de ROIs e defini√ß√£o de tipos/estados. Fornece primitivas reutiliz√°veis
para detector e tracker.

RESPONSABILIDADES:
- Definir enum de estados de tracking (ConeState)
- Opera√ß√µes matem√°ticas b√°sicas (clamp)
- Opera√ß√µes com bounding boxes (uni√£o, centro, dist√¢ncia, sobreposi√ß√£o)
- Extra√ß√£o segura de ROIs com clipping

ENUMERA√á√ïES:

Enum: ConeState
---------------
Descri√ß√£o: Estados do ciclo de vida de um track
Valores:
  - SUSPECT: Track rec√©m-criado ou com baixa confian√ßa (n√£o confirmado)
  - CONFIRMED: Track confirmado (frames suficientes + score alto)
  - LOST: Track perdido (n√£o usado atualmente, tracks s√£o deletados em vez disso)

FUN√á√ïES PRINCIPAIS:

Fun√ß√£o: clamp(v: float, lo: float, hi: float) -> float
  - Descri√ß√£o: Limita valor v entre lo e hi
  - Par√¢metros:
    * v: Valor a limitar
    * lo: Limite inferior
    * hi: Limite superior
  - Retorno: max(lo, min(hi, v))
  - Exemplo: clamp(150, 0, 100) = 100, clamp(-5, 0, 100) = 0

Fun√ß√£o: safe_roi(img: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, Tuple[int, int, int, int]]
  - Descri√ß√£o: Extrai ROI de imagem com clipping seguro
  - Par√¢metros:
    * img: Imagem numpy (HxW ou HxWxC)
    * bbox: Bounding box (x, y, w, h)
  - Retorno: Tupla (roi, bbox_clipped) onde:
    * roi: Regi√£o extra√≠da (subarray da imagem)
    * bbox_clipped: Bbox ap√≥s clipping (x, y, w, h)
  - Comportamento:
    1. Obt√©m dimens√µes da imagem (h, w)
    2. Clippa coordenadas do bbox para estar dentro da imagem:
       - x = clamp(x, 0, w-1)
       - y = clamp(y, 0, h-1)
       - x2 = clamp(x+bw, 0, w)
       - y2 = clamp(y+bh, 0, h)
    3. Se bbox inv√°lido (x2 <= x ou y2 <= y): retorna ROI vazio e bbox (0,0,0,0)
    4. Extrai roi = img[y:y2, x:x2]
    5. Retorna (roi, (x, y, x2-x, y2-y))
  - Uso: Evita erros de indexa√ß√£o ao extrair regi√µes pr√≥ximas √†s bordas

Fun√ß√£o: x_overlap_ratio(a: Tuple[int, int, int, int], b: Tuple[int, int, int, int]) -> float
  - Descri√ß√£o: Calcula raz√£o de sobreposi√ß√£o horizontal entre dois bboxes
  - Par√¢metros:
    * a, b: Bounding boxes (x, y, w, h)
  - Retorno: Sobreposi√ß√£o normalizada (0.0 a 1.0)
  - Algoritmo:
    1. Extrai a1 = ax, a2 = ax + aw (limites horizontais de a)
    2. Extrai b1 = bx, b2 = bx + bw (limites horizontais de b)
    3. Calcula interse√ß√£o: inter = max(0, min(a2, b2) - max(a1, b1))
    4. Normaliza pelo menor width: ratio = inter / min(aw, bw)
    5. Retorna ratio (0.0 = sem sobreposi√ß√£o, 1.0 = total)
  - Uso: Agrupar partes verticalmente alinhadas no detector

Fun√ß√£o: bbox_union(boxes: List[Tuple[int, int, int, int]]) -> Optional[Tuple[int, int, int, int]]
  - Descri√ß√£o: Calcula bounding box uni√£o de m√∫ltiplos boxes
  - Par√¢metros:
    * boxes: Lista de bounding boxes [(x, y, w, h), ...]
  - Retorno: Bbox uni√£o (x_min, y_min, w, h) ou None se lista vazia
  - Algoritmo:
    1. Se boxes vazio: retorna None
    2. Calcula x_min = min de todos x
    3. Calcula y_min = min de todos y
    4. Calcula x_max = max de todos (x + w)
    5. Calcula y_max = max de todos (y + h)
    6. w = x_max - x_min
    7. h = y_max - y_min
    8. Retorna (x_min, y_min, w, h)
  - Uso: Agrupar m√∫ltiplas partes em um √∫nico bbox no detector

Fun√ß√£o: bbox_center(b: Tuple[int, int, int, int]) -> Tuple[float, float]
  - Descri√ß√£o: Calcula centro de um bounding box
  - Par√¢metros:
    * b: Bounding box (x, y, w, h)
  - Retorno: (center_x, center_y) em float
  - Algoritmo: (x + w/2, y + h/2)

Fun√ß√£o: bbox_distance(a: Tuple[int, int, int, int], b: Tuple[int, int, int, int]) -> float
  - Descri√ß√£o: Calcula dist√¢ncia euclidiana entre centros de dois bboxes
  - Par√¢metros:
    * a, b: Bounding boxes (x, y, w, h)
  - Retorno: Dist√¢ncia em pixels (float)
  - Algoritmo:
    1. (ax, ay) = bbox_center(a)
    2. (bx, by) = bbox_center(b)
    3. dist = sqrt((ax - bx)¬≤ + (ay - by)¬≤)
    4. Retorna dist
  - Uso: Associa√ß√£o de tracks no tracker (menor dist√¢ncia = melhor match)

CONEX√ïES COM OUTROS COMPONENTES:
- USADO POR: cone_tracker.detector (safe_roi, bbox_union, x_overlap_ratio, clamp)
- USADO POR: cone_tracker.tracker (bbox_distance, ConeState)
- USADO POR: cone_tracker.visualizer (ConeState)
- EXPORTADO VIA: cone_tracker/__init__.py (ConeState)

DEPEND√äNCIAS EXTERNAS:
- numpy: Para c√°lculo de hipot (dist√¢ncia euclidiana)
- enum: Para defini√ß√£o de ConeState

PONTOS DE ATEN√á√ÉO:
1. safe_roi retorna bbox CLIPPED (pode ser diferente do bbox input)
2. x_overlap_ratio normaliza pelo MENOR width (n√£o pela uni√£o)
3. bbox_distance usa centros (n√£o considera tamanho dos boxes)
4. ConeState.LOST n√£o √© usado no c√≥digo atual (tracks s√£o deletados diretamente)

LIMITA√á√ïES:
- N√£o suporta bboxes rotacionados (apenas axis-aligned)
- bbox_distance n√£o considera tamanho/forma dos boxes (apenas centros)
- N√£o implementa IoU (Intersection over Union), que seria mais robusto

RECOMENDA√á√ïES T√âCNICAS:
1. Para melhor associa√ß√£o, considerar implementar IoU al√©m de dist√¢ncia de centros
2. Se precisar de bboxes rotacionados, migrar para cv2.RotatedRect
3. Documentar claramente que x_overlap_ratio normaliza pelo menor width


--------------------------------------------------------------------------------
5. M√ìDULO: cone_tracker/color_utils.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/color_utils.py

DESCRI√á√ÉO DETALHADA:
Utilit√°rios para processamento de cor e cria√ß√£o de m√°scaras alternativas de
segmenta√ß√£o. Fornece t√©cnicas complementares ao threshold HSV b√°sico para
maior robustez em diferentes condi√ß√µes de ilumina√ß√£o.

RESPONSABILIDADES:
- Normaliza√ß√£o de cor via gray-world
- Segmenta√ß√£o por chromaticity r/g normalizada
- Carregamento e aplica√ß√£o de histogramas para backprojection
- Gera√ß√£o de m√°scaras bin√°rias a partir de diferentes espa√ßos de cor

FUN√á√ïES PRINCIPAIS:

Fun√ß√£o: gray_world(bgr: np.ndarray) -> np.ndarray
  - Descri√ß√£o: Aplica normaliza√ß√£o de cor gray-world
  - Par√¢metros:
    * bgr: Imagem BGR (uint8, HxWx3)
  - Retorno: Imagem BGR normalizada (uint8, HxWx3)
  - Algoritmo:
    1. Converte para float32
    2. Calcula m√©dia de cada canal: avg_b, avg_g, avg_r
    3. Calcula m√©dia geral: avg_gray = mean(avg_b, avg_g, avg_r)
    4. Calcula fatores de escala: scale_c = avg_gray / (avg_c + 1e-6)
    5. Multiplica cada canal pelo respectivo fator
    6. Clippa para [0, 255] e converte para uint8
  - Objetivo: Remover domin√¢ncia de cor (cast de ilumina√ß√£o), assumindo
    que a m√©dia da cena deve ser cinza neutro
  - Uso: Habilitar via color.enable_gray_world para normalizar antes de HSV

Fun√ß√£o: rg_chromaticity_mask(bgr: np.ndarray, thresholds: Dict[str, float]) -> np.ndarray
  - Descri√ß√£o: Cria m√°scara baseada em chromaticity r e g normalizada
  - Par√¢metros:
    * bgr: Imagem BGR (uint8, HxWx3)
    * thresholds: Dict com r_min, r_max, g_min, g_max (valores 0-1)
  - Retorno: M√°scara bin√°ria (uint8, 0 ou 255)
  - Algoritmo:
    1. Converte para float32
    2. Calcula soma dos canais: s = B + G + R
    3. Normaliza: r = R / (s + 1e-6), g = G / (s + 1e-6)
    4. Cria condi√ß√£o: (r >= r_min) AND (r <= r_max) AND (g >= g_min) AND (g <= g_max)
    5. Converte booleano para uint8 (0 ou 255)
  - Objetivo: Segmenta√ß√£o invariante a intensidade (foca em cor pura)
  - Uso: Habilitar via color.enable_rg para complementar HSV

Fun√ß√£o: load_backproj_hist(path: str)
  - Descri√ß√£o: Carrega histograma 2D (Hue-Saturation) de arquivo .npy
  - Par√¢metros:
    * path: Caminho para arquivo .npy
  - Retorno: Histograma normalizado (numpy array 2D, float32, 0-255) ou None
  - Comportamento:
    1. Verifica se path existe
    2. Carrega via np.load(path)
    3. Converte para float32
    4. Normaliza com cv2.normalize(hist, 0, 255, NORM_MINMAX)
    5. Retorna histograma
    6. Em caso de erro: registra warning e retorna None
  - Uso: Pr√©-carregar histograma de cones conhecidos para backprojection

Fun√ß√£o: mask_from_backproj(hsv: np.ndarray, hist: np.ndarray, thresh: int = 50) -> np.ndarray
  - Descri√ß√£o: Gera m√°scara via backprojection de histograma H-S
  - Par√¢metros:
    * hsv: Imagem HSV (uint8, HxWx3)
    * hist: Histograma 2D pr√©-carregado (float32)
    * thresh: Threshold para binariza√ß√£o (int, 0-255, default 50)
  - Retorno: M√°scara bin√°ria (uint8, 0 ou 255)
  - Algoritmo:
    1. Se hist √© None: retorna m√°scara zerada
    2. Calcula backprojection: back = calcBackProject([hsv], [0,1], hist, [0,180,0,256], 1)
       - Canais 0,1 = Hue e Saturation
       - Ranges: Hue 0-180, Saturation 0-256
    3. Aplica threshold: mask = threshold(back, thresh, 255, THRESH_BINARY)
    4. Retorna mask
  - Objetivo: Detectar pixels com cor similar ao histograma (treino com exemplos positivos)
  - Uso: Habilitar via color.enable_backproj ap√≥s gerar histograma de cones

CONEX√ïES COM OUTROS COMPONENTES:
- USADO POR: cone_tracker.detector (gray_world, rg_chromaticity_mask, load_backproj_hist, mask_from_backproj)

DEPEND√äNCIAS EXTERNAS:
- cv2: calcBackProject, threshold, normalize
- numpy: Arrays, opera√ß√µes matem√°ticas
- os: Verifica√ß√£o de exist√™ncia de arquivo

PONTOS DE ATEN√á√ÉO:
1. gray_world assume cena com equil√≠brio de cores (falha em cenas monocrom√°ticas)
2. rg chromaticity perde informa√ß√£o de intensidade (√∫til mas n√£o suficiente sozinho)
3. Backprojection requer histograma pr√©-gerado (n√£o incluso no repo)
4. Histograma deve ser 2D (Hue-Saturation), n√£o 3D
5. Threshold de backprojection (default 50) precisa ajuste por cen√°rio

LIMITA√á√ïES:
- gray_world n√£o corrige ilumina√ß√£o n√£o-uniforme (spotlight, sombras)
- rg chromaticity sens√≠vel a ru√≠do em pixels escuros (divis√£o por soma pequena)
- Backprojection requer coleta de amostras positivas (workflow manual)
- N√£o implementa m√©todos avan√ßados (Retinex, color constancy neural)

RECOMENDA√á√ïES T√âCNICAS:
1. gray_world √© √∫til em outdoor com ilumina√ß√£o natural (sol, nublado)
2. Desabilitar gray_world em indoor com ilumina√ß√£o artificial balanceada
3. Para gerar histograma de backprojection:
   a. Capturar frames com cones conhecidos
   b. Anotar manualmente regi√µes de cone
   c. Calcular histograma H-S das regi√µes
   d. Salvar com np.save("cone_hist.npy", hist)
4. Ajustar backproj_thresh empiricamente (50-100 t√≠pico)
5. rg chromaticity √∫til para laranja (alto r, m√©dio g)
6. Combinar m√∫ltiplas m√°scaras via OR para robustez

--------------------------------------------------------------------------------
6. M√ìDULO: cone_tracker/config.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/config.py

DESCRI√á√ÉO DETALHADA:
Gerenciamento centralizado de configura√ß√£o do sistema via arquivos YAML.
Fornece configura√ß√£o padr√£o (DEFAULT_CONFIG), carregamento com merge profundo,
salvamento e hot-reload (detec√ß√£o autom√°tica de mudan√ßas).

RESPONSABILIDADES:
- Definir configura√ß√£o padr√£o completa (DEFAULT_CONFIG)
- Carregar configura√ß√£o de arquivo YAML
- Fazer merge profundo de configura√ß√£o de usu√°rio com defaults
- Salvar configura√ß√£o atual para YAML
- Monitorar arquivo de config para mudan√ßas (hot-reload)

CONSTANTES:

DEFAULT_CONFIG: Dict[str, Any]
------------------------------
Descri√ß√£o: Configura√ß√£o padr√£o completa com todos os par√¢metros documentados
Se√ß√µes principais:
  - camera: Configura√ß√£o de captura (√≠ndice, resolu√ß√µes, FPS, v√≠deo, HFOV)
  - debug: Op√ß√µes de visualiza√ß√£o e logging
  - hsv_orange: Ranges HSV para detec√ß√£o de laranja (2 ranges)
  - morphology: Par√¢metros de opera√ß√µes morfol√≥gicas
  - grouping: Par√¢metros de agrupamento de partes
  - geometry: Valida√ß√£o geom√©trica e thresholds
  - weights: Pesos para c√°lculo de score
  - tracking: Par√¢metros do tracker multi-objeto
  - clahe: Configura√ß√£o de equaliza√ß√£o de histograma
  - color: Op√ß√µes de processamento de cor avan√ßado

Valores not√°veis:
  - camera.process_width/height: 960x540 (processamento)
  - camera.capture_width/height: 1920x1080 (captura)
  - camera.fps: 30
  - camera.hfov_deg: 70.0 (campo de vis√£o horizontal)
  - geometry.aspect_min/max: 1.0 a 6.0 (cones variam de quadrados a muito altos)
  - tracking.lost_timeout: 0.6s (MUITO CURTO - recomenda-se 3.0s)
  - tracking.association_max_distance: 140 pixels
  - tracking.min_frames_for_confirm: 6 frames

FUN√á√ïES PRINCIPAIS:

Fun√ß√£o: deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]
  - Descri√ß√£o: Faz merge recursivo de dois dicion√°rios
  - Par√¢metros:
    * base: Dicion√°rio base (padr√µes)
    * override: Dicion√°rio com overrides (config do usu√°rio)
  - Retorno: Novo dicion√°rio merged
  - Algoritmo:
    1. Copia base para out
    2. Para cada (k, v) em override:
       a. Se v √© dict E out[k] √© dict: recursivamente merge
       b. Sen√£o: sobrescreve out[k] = v
    3. Retorna out
  - Comportamento: Override tem preced√™ncia, mas preserva chaves n√£o especificadas

Fun√ß√£o: load_config(path: str = "cone_config.yaml") -> Dict[str, Any]
  - Descri√ß√£o: Carrega configura√ß√£o de YAML com merge de defaults
  - Par√¢metros:
    * path: Caminho para arquivo YAML (default "cone_config.yaml")
  - Retorno: Dicion√°rio de configura√ß√£o completo
  - Fluxo:
    1. Inicializa cfg = c√≥pia de DEFAULT_CONFIG
    2. Se arquivo n√£o existe: retorna DEFAULT_CONFIG
    3. Tenta abrir e parsear YAML (yaml.safe_load)
    4. Se sucesso: cfg = deep_merge(DEFAULT_CONFIG, user_config)
    5. Se erro: registra exception, retorna DEFAULT_CONFIG
    6. Faz segundo merge: cfg = deep_merge(DEFAULT_CONFIG, cfg)
       (garantir que novos campos em DEFAULT apare√ßam)
    7. Retorna cfg
  - Tratamento de erros: Logs exception mas n√£o falha (usa defaults)

Fun√ß√£o: save_config(config: Dict[str, Any], path: str = "cone_config.yaml") -> None
  - Descri√ß√£o: Salva configura√ß√£o para arquivo YAML
  - Par√¢metros:
    * config: Dicion√°rio de configura√ß√£o
    * path: Caminho de destino (default "cone_config.yaml")
  - Retorno: None
  - Comportamento:
    1. Abre arquivo para escrita
    2. Serializa via yaml.dump(config, default_flow_style=False, sort_keys=False)
    3. Registra log "Config saved to {path}"
    4. Se erro: registra exception

Vari√°vel global: _config_mtime: Dict[str, float]
  - Descri√ß√£o: Cache de timestamps de modifica√ß√£o de arquivos (para hot-reload)
  - Estrutura: {path: mtime}

Fun√ß√£o: watch_config(path: str = "cone_config.yaml") -> bool
  - Descri√ß√£o: Verifica se arquivo de config foi modificado desde √∫ltima chamada
  - Par√¢metros:
    * path: Caminho do arquivo a monitorar
  - Retorno: True se modificado, False caso contr√°rio
  - Algoritmo:
    1. Se arquivo n√£o existe: retorna False
    2. Obt√©m mtime atual via os.path.getmtime(path)
    3. Se primeira vez monitorando:
       - Armazena mtime em _config_mtime[path]
       - Retorna False (sem mudan√ßa na primeira chamada)
    4. Se mtime > mtime_armazenado:
       - Atualiza _config_mtime[path] = mtime
       - Retorna True (arquivo mudou)
    5. Sen√£o: retorna False
  - Uso: Chamado no loop principal de app.py para hot-reload

FLUXO DE HOT-RELOAD:
```
// No app.py, dentro do loop:
se watch_config("cone_config.yaml") == True:
  app.reload_config()  // Recarrega tudo
```

CONEX√ïES COM OUTROS COMPONENTES:
- USADO POR: Todos os m√≥dulos (detector, tracker, visualizer, app)
- EXPORTADO VIA: cone_tracker/__init__.py (load_config, save_config, DEFAULT_CONFIG)

DEPEND√äNCIAS EXTERNAS:
- yaml: Serializa√ß√£o/deserializa√ß√£o YAML
- os: Verifica√ß√£o de exist√™ncia e mtime de arquivos

PONTOS DE ATEN√á√ÉO:
1. DUPLO MERGE em load_config: Primeiro user->default, depois default->resultado.
   Garante novos campos de DEFAULT aparecerem mesmo em configs antigos.
2. watch_config retorna False na primeira chamada (inicializa√ß√£o do cache)
3. Hot-reload REINICIALIZA componentes (detector, tracker, visualizer), perdendo
   estado de tracking.
4. save_config sobrescreve arquivo completamente (n√£o faz merge parcial)
5. YAML parser pode falhar com sintaxe incorreta (usa defaults como fallback)

LIMITA√á√ïES:
- N√£o valida tipos ou ranges de valores (aceita qualquer YAML)
- N√£o suporta m√∫ltiplos arquivos de config (profiles)
- Hot-reload perde estado de tracking (n√£o persiste)
- N√£o notifica sobre campos desconhecidos (podem indicar typos)
- N√£o suporta vari√°veis de ambiente ou interpola√ß√£o

RECOMENDA√á√ïES T√âCNICAS:
1. Validar configura√ß√£o ap√≥s load (ex: asserts para ranges cr√≠ticos)
2. Documentar todas as op√ß√µes em cone_config.yaml.example
3. Para produ√ß√£o, considerar schemas (ex: pydantic, cerberus) para valida√ß√£o
4. Implementar config profiles (dev, prod, test) via arquivos separados
5. Adicionar versioning de config para migra√ß√£o autom√°tica
6. Logar diferen√ßas entre DEFAULT e user config no startup
7. Considerar format-preserving YAML library (ruamel.yaml) para preservar
   coment√°rios ao salvar

--------------------------------------------------------------------------------
7. M√ìDULO: cone_tracker/visualizer.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: cone_tracker/visualizer.py

DESCRI√á√ÉO DETALHADA:
Respons√°vel por renderizar visualiza√ß√µes de detec√ß√£o e tracking sobre frames.
Desenha bounding boxes, labels, FPS, contadores e informa√ß√µes de debug.

RESPONSABILIDADES:
- Desenhar bounding boxes coloridos por track ID
- Exibir labels com ID, estado e score
- Renderizar overlay de heading (LEFT/RIGHT/CENTER ¬±deg) para tracks CONFIRMED quando habilitado
- Renderizar informa√ß√µes de FPS e contadores
- Mostrar rejei√ß√µes (com motivo) se habilitado
- Exibir mensagem de reload de config tempor√°ria

CLASSES E M√âTODOS PRINCIPAIS:

Classe: Visualizer
------------------
Assinatura: Visualizer(config: Dict[str, Any])

Atributo: COLORS
  - Descri√ß√£o: Lista de cores BGR para tracks
  - Valor: [(0,255,0), (0,200,255), (255,0,0), (255,255,0), (255,0,255), (0,255,255), (180,180,0), (0,180,180)]
  - Uso: Cores rotacionadas por track_id % len(COLORS)

M√©todo: __init__(self, config: Dict[str, Any])
  - Descri√ß√£o: Inicializa visualizador com configura√ß√£o
  - Par√¢metros:
    * config: Dicion√°rio de configura√ß√£o completo
  - Comportamento: Extrai se√ß√£o debug da config

M√©todo: _color(self, track_id: int) -> Tuple[int, int, int]
  - Descri√ß√£o: Retorna cor para um track ID
  - Par√¢metros:
    * track_id: ID do track
  - Retorno: Cor BGR (tupla)
  - Algoritmo: COLORS[track_id % len(COLORS)]

M√©todo: draw(self, frame: np.ndarray, tracks: List[Track], rejects: List[Tuple[Tuple[int, int, int, int], str]], fps: float, config_reload_msg: str = None) -> np.ndarray
  - Descri√ß√£o: Desenha todas as visualiza√ß√µes sobre o frame
  - Par√¢metros:
    * frame: Frame BGR (numpy array, ser√° modificado)
    * tracks: Lista de tracks para desenhar
    * rejects: Lista de detec√ß√µes rejeitadas (bbox, motivo)
    * fps: FPS atual
    * config_reload_msg: Mensagem de reload (opcional, exibida temporariamente)
  - Retorno: Frame anotado (mesmo array modificado)
  
  - Renderiza√ß√µes (em ordem):
    
    1. FPS (canto superior esquerdo):
       - Texto: "FPS: {fps:.1f}"
       - Posi√ß√£o: (10, 30)
       - Cor: Verde (0,255,0)
       - Font: FONT_HERSHEY_SIMPLEX, escala 0.7, espessura 2
    
    2. Contador de Tracks:
       - Calcula confirmed_count = tracks com state==CONFIRMED
       - Calcula suspect_count = tracks com state==SUSPECT
       - Texto: "Tracks: {total} ({confirmed} conf, {suspect} susp)"
       - Posi√ß√£o: (10, 60)
       - Cor: Verde (0,255,0)
    
    3. Contador de Rejei√ß√µes:
       - Texto: "Rejects: {len(rejects)}"
       - Posi√ß√£o: (10, 90)
       - Cor: Vermelha (0,0,255)
    
    4. Mensagem de Reload (se config_reload_msg n√£o None):
       - Texto: config_reload_msg (ex: "‚öôÔ∏è Config recarregada!")
       - Posi√ß√£o: (10, 120)
       - Cor: Ciano (0,255,255)
       - Nota: Exibida por 3 segundos (controlado em app.py)
    
    5. Bounding Boxes de Tracks:
       Para cada track em tracks:
         a. Se track.state == CONFIRMED:
            - cor = self._color(track.track_id)
         b. Sen√£o (SUSPECT):
            - Se draw_suspects habilitado:
              + cor = Amarelo (0,255,255)
            - Sen√£o: skip (n√£o desenha)
         
         c. Desenha ret√¢ngulo:
            - cv2.rectangle(frame, (x,y), (x+w, y+h), cor, espessura=2)
         
         d. Desenha labels acima do ret√¢ngulo:
             
             Para tracks CONFIRMED com debug.show_heading_overlay habilitado:
               LINHA 1 (Heading overlay):
                 - Calcula heading conforme descrito abaixo em "C√ÅLCULO DE HEADING"
                 - Texto: "{LEFT|RIGHT|CENTER} {¬±X.XX}¬∞"
                   Exemplos: "LEFT -3.12¬∞", "RIGHT +2.45¬∞", "CENTER +0.12¬∞"
                 - Posi√ß√£o: (x, y-30) ou ajustado se muito pr√≥ximo da borda superior
                 - Cor: mesma do ret√¢ngulo do track
                 - Font: FONT_HERSHEY_SIMPLEX, escala 0.45, espessura 2
               
               LINHA 2 (Info do track):
                 - Texto: "ID {id} {state.name} avg={avg_score:.2f}"
                 - Posi√ß√£o: (x, y-10) ou ajustado se muito pr√≥ximo da borda superior
                 - Cor: mesma do ret√¢ngulo
                 - Font: FONT_HERSHEY_SIMPLEX, escala 0.45, espessura 2
             
             Para tracks CONFIRMED com debug.show_heading_overlay desabilitado:
               LINHA √öNICA (Info do track):
                 - Texto: "ID {id} {state.name} avg={avg_score:.2f}"
                 - Posi√ß√£o: (x, y-10) ou (x, 0) se y muito pequeno
                 - Cor: mesma do ret√¢ngulo
                 - Font: FONT_HERSHEY_SIMPLEX, escala 0.45, espessura 2
             
             Para tracks SUSPECT (se draw_suspects habilitado):
               LINHA √öNICA (Info do track):
                 - Texto: "ID {id} {state.name} avg={avg_score:.2f}"
                 - Posi√ß√£o: (x, y-10)
                 - Cor: Amarelo (0,255,255)
                 - Font: FONT_HERSHEY_SIMPLEX, escala 0.45, espessura 2
                 - Nota: SUSPECT tracks N√ÉO recebem heading overlay, apenas CONFIRMED
    
    6. Rejei√ß√µes (se show_rejection_reason habilitado):
       Para cada (bbox, motivo) em rejects:
         a. Desenha ret√¢ngulo vermelho:
            - cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), espessura=2)
         
         b. Desenha motivo acima:
            - Texto: motivo (ex: "score=0.28")
            - Posi√ß√£o: (x, y-5) ou (x, 0) se y muito pequeno
            - Cor: Vermelha (0,0,255)
            - Font: FONT_HERSHEY_SIMPLEX, escala 0.5, espessura 2

C√ÅLCULO DE HEADING (para overlay visual):
-----------------------------------------

Quando debug.show_heading_overlay est√° habilitado, o visualizador calcula e exibe
a dire√ß√£o (LEFT/RIGHT/CENTER) e o erro angular em graus para cada track CONFIRMED.
Este c√°lculo segue exatamente a mesma l√≥gica implementada em app._debug_print_heading
(que imprime no terminal), mas aplicado ao overlay visual.

MATEM√ÅTICA DO HEADING:

1. ENTRADA:
   - track.cx: Centro horizontal do bbox (em pixels)
   - frame_w: Largura do frame (em pixels)
   - camera.hfov_deg: Campo de vis√£o horizontal da c√¢mera (graus, default 70.0)

2. VALIDA√á√ÉO DE HFOV:
   - Range v√°lido: 10.0 <= hfov_deg <= 170.0
   - Se hfov_deg fora do range ou n√£o configurado: usa fallback de 70.0

3. C√ÅLCULO DO FOCAL LENGTH (modelo pinhole):
   - frame_center_x = frame_w / 2.0
   - hfov_rad = hfov_deg * (œÄ / 180)
   - focal_px = (frame_w / 2.0) / tan(hfov_rad / 2.0)

4. C√ÅLCULO DO ERRO ANGULAR:
   - err_px = track.cx - frame_center_x  (erro horizontal em pixels)
   - err_deg = atan(err_px / focal_px) * (180 / œÄ)  (convers√£o para graus)

5. DECIS√ÉO DE DIRE√á√ÉO:
   - Se |err_deg| < debug.heading_center_deadband_deg:
     + Dire√ß√£o = "CENTER"
   - Sen√£o, se err_deg > 0:
     + Dire√ß√£o = "RIGHT"  (cone √† direita do centro)
   - Sen√£o:
     + Dire√ß√£o = "LEFT"   (cone √† esquerda do centro)

6. FORMATA√á√ÉO DO LABEL:
   - err_deg sempre mostrado com sinal e 2 decimais: "+3.12¬∞", "-1.80¬∞", "+0.12¬∞"
   - Label final: "{Dire√ß√£o} {err_deg}¬∞"
   - Exemplos:
     + "LEFT -3.12¬∞"
     + "RIGHT +2.45¬∞"
     + "CENTER +0.12¬∞"

NOTAS IMPORTANTES:
- Este c√°lculo √© id√™ntico ao usado em app._debug_print_heading (terminal)
- A diferen√ßa √© que debug.print_heading controla logs de terminal, enquanto
  debug.show_heading_overlay controla overlay visual
- Ambos podem ser habilitados independentemente
- O overlay visual √© desenhado apenas para tracks CONFIRMED
- O par√¢metro debug.heading_center_deadband_deg (default 0.5) define o threshold
  em graus para considerar um cone como CENTER (dentro da deadband)

CONEX√ïES COM OUTROS COMPONENTES:
- USADO POR: app.App (self.vis.draw())
- IMPORTA: tracker.Track, utils.ConeState
- EXPORTADO VIA: cone_tracker/__init__.py

PAR√ÇMETROS CONFIGUR√ÅVEIS:
- debug.draw_suspects: Desenhar tracks SUSPECT em amarelo (bool, default false)
- debug.show_rejection_reason: Desenhar rejei√ß√µes em vermelho (bool, default false)
- debug.show_heading_overlay: Quando true, desenha linha de heading (LEFT/RIGHT/CENTER ¬±deg) acima de cada bbox CONFIRMED (bool, default false)
- debug.heading_center_deadband_deg: Deadband threshold em graus; dentro desse erro absoluto o r√≥tulo se torna CENTER (float, default 0.5)
- camera.hfov_deg: Campo de vis√£o horizontal da c√¢mera em graus (float, default 70.0, range v√°lido 10-170)

DEPEND√äNCIAS EXTERNAS:
- cv2: putText, rectangle
- numpy: Arrays

PONTOS DE ATEN√á√ÉO:
1. MODIFICA√á√ÉO IN-PLACE: Frame √© modificado diretamente (retorna mesmo array)
2. ORDEM DE CORES: Cores rotacionam por ID. Tracks deletados e recriados podem
   reutilizar cores (IDs crescem infinitamente).
3. SUSPECTS S√ì APARECEM SE HABILITADO: Default √© draw_suspects=false (s√≥ confirmed)
4. REJEI√á√ïES: Pode poluir visualiza√ß√£o se muitas. Usar apenas para debug.
5. FPS POSITION: Informa√ß√µes sempre desenhadas, mesmo se show_windows=false
   (importante para output_video_path)
6. HEADING OVERLAY: Apenas para tracks CONFIRMED quando show_heading_overlay=true.
   Tracks SUSPECT nunca recebem heading overlay. O overlay usa duas linhas acima
   do bbox, requerendo mais espa√ßo vertical.

LIMITA√á√ïES:
- Cores fixas (8 cores apenas, repetem ap√≥s 8 tracks)
- N√£o desenha trajet√≥rias (hist√≥rico de posi√ß√µes)
- N√£o exibe m√©tricas adicionais (velocidade, acelera√ß√£o)
- Labels podem sobrepor se tracks pr√≥ximos
- N√£o suporta zoom ou pan interativo

RECOMENDA√á√ïES T√âCNICAS:
1. Para mais de 8 tracks simult√¢neos, expandir paleta COLORS
2. Implementar gera√ß√£o din√¢mica de cores (hashing de ID -> HSV)
3. Adicionar op√ß√£o de desenhar trajet√≥ria (√∫ltimas N posi√ß√µes)
4. Implementar overlay semi-transparente para melhor legibilidade
5. Adicionar op√ß√£o de tamanho de fonte configur√°vel
6. Considerar biblioteca de visualiza√ß√£o mais avan√ßada (ex: vis_utils do TensorFlow)

--------------------------------------------------------------------------------
8. M√ìDULO: test7.py
--------------------------------------------------------------------------------

CAMINHO RELATIVO: test7.py (raiz do reposit√≥rio)

DESCRI√á√ÉO DETALHADA:
Ponto de entrada principal da aplica√ß√£o. Script minimalista que configura
logging e inicia a aplica√ß√£o via cone_tracker.App.

RESPONSABILIDADES:
- Configurar logging b√°sico (level INFO, formato com timestamp)
- Importar e instanciar App
- Executar loop principal

C√ìDIGO COMPLETO:
```python
#!/usr/bin/env python3
"""
Main entry point for cone detection and tracking system.
This script uses the modular cone_tracker package.
"""
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from cone_tracker import App

if __name__ == "__main__":
    App().run()
```

FLUXO DE EXECU√á√ÉO:
1. Configura logging com level INFO e formato detalhado
2. Importa classe App de cone_tracker
3. Se executado como script principal:
   a. Instancia App() (carrega config, cria componentes)
   b. Chama App().run() (inicia loop principal)
   c. Loop roda at√© usu√°rio pressionar 'q' ou erro fatal

COMO EXECUTAR:
```bash
# Execu√ß√£o b√°sica
python3 test7.py

# Com config customizado (editar cone_config.yaml antes)
python3 test7.py

# Exemplo com v√≠deo de entrada
# (configurar camera.video_path em cone_config.yaml)
python3 test7.py

# Exemplo headless (salvar v√≠deo processado)
# (configurar camera.output_video_path e debug.show_windows=false)
python3 test7.py
```

CONEX√ïES COM OUTROS COMPONENTES:
- IMPORTA: cone_tracker.App
- PONTO DE ENTRADA: Executado pelo usu√°rio ou scripts de automa√ß√£o

DEPEND√äNCIAS EXTERNAS:
- logging: Configura√ß√£o de logs
- cone_tracker: Pacote principal

PONTOS DE ATEN√á√ÉO:
1. LOGGING GLOBAL: Configura√ß√£o de logging afeta TODOS os m√≥dulos que usam
   logging padr√£o do Python.
2. SHEBANG: #!/usr/bin/env python3 permite execu√ß√£o direta (chmod +x test7.py)
3. NOME "test7.py": Nome legado, deveria ser renomeado para main.py ou run_tracker.py
4. SEM ARGUMENTOS CLI: N√£o aceita argumentos de linha de comando (tudo via config YAML)

LIMITA√á√ïES:
- N√£o suporta argumentos CLI (--config, --video, --output, etc.)
- N√£o implementa signal handlers (Ctrl+C pode deixar c√¢mera aberta)
- Logging sempre em INFO (n√£o configur√°vel via CLI)
- N√£o valida ambiente (OpenCV, GPU, etc.) antes de rodar

RECOMENDA√á√ïES T√âCNICAS:
1. Renomear para main.py ou run_cone_tracker.py
2. Adicionar argparse para:
   - Especificar arquivo de config: --config path/to/config.yaml
   - Override de par√¢metros: --video input.mp4 --output result.mp4
   - Level de logging: --log-level DEBUG
3. Implementar signal handler (SIGINT, SIGTERM) para cleanup graceful
4. Adicionar valida√ß√£o de ambiente:
   - Verificar se OpenCV foi compilado com GUI (cv2.getBuildInformation())
   - Verificar se c√¢mera est√° dispon√≠vel antes de rodar
5. Implementar modo dry-run (--dry-run) para validar config sem executar


================================================================================
ARQUITETURA E FLUXOS DE DADOS
================================================================================

VIS√ÉO GERAL DA ARQUITETURA:
---------------------------
O sistema segue arquitetura modular em camadas com separa√ß√£o clara de responsabilidades:

CAMADA 1 - ENTRADA (Input Layer):
  - Captura de v√≠deo/c√¢mera (OpenCV VideoCapture)
  - Suporte a m√∫ltiplas fontes (c√¢mera USB, arquivos MP4, streams RTSP)
  - Redimensionamento autom√°tico para resolu√ß√£o de processamento

CAMADA 2 - DETEC√á√ÉO (Detection Layer):
  - Pr√©-processamento de imagem (blur, CLAHE, normaliza√ß√£o de cor)
  - Segmenta√ß√£o multi-modal (HSV, Lab, R/G, backprojection)
  - Opera√ß√µes morfol√≥gicas (abertura, fechamento)
  - Extra√ß√£o de contornos e partes
  - Agrupamento de partes conectadas
  - Valida√ß√£o geom√©trica rigorosa
  - C√°lculo de confidence scores

CAMADA 3 - TRACKING (Tracking Layer):
  - Associa√ß√£o de detec√ß√µes a tracks existentes (greedy algorithm)
  - Cria√ß√£o de novos tracks
  - Atualiza√ß√£o de posi√ß√µes via EMA
  - Gerenciamento de estados (SUSPECT ‚Üí CONFIRMED)
  - Dele√ß√£o de tracks perdidos

CAMADA 4 - SA√çDA (Output Layer):
  - Visualiza√ß√£o de resultados
  - C√°lculo de heading/steering (√¢ngulos e dist√¢ncias)
  - Logs estruturados
  - Grava√ß√£o de v√≠deo processado
  - Interface GUI (opcional)

FLUXO DE DADOS COMPLETO (DO SENSOR AT√â A A√á√ÉO):
-----------------------------------------------

FRAME n (tempo t):
  |
  v
[1. CAPTURA]
  - VideoCapture.read() -> frame_raw (1920x1080 BGR)
  - cv2.resize() -> frame_proc (960x540 BGR)
  |
  v
[2. PR√â-PROCESSAMENTO - detector.preprocess()]
  - Se gray_world: normalizar cores
  - GaussianBlur(5x5) -> frame_blur
  - cvtColor(BGR->HSV) -> frame_hsv
  - Split canais: H, S, V
  - CLAHE no canal V -> V_eq
  - Merge: frame_hsv_eq
  |
  v
[3. SEGMENTA√á√ÉO - detector.get_mask()]
  - M√°scara HSV (2 ranges para laranja): mask_hsv
  - Se Lab habilitado: mask_lab
  - Se RG habilitado: mask_rg
  - Se backproj habilitado: mask_bp
  - Combinar: mask_combined = OR(todas as m√°scaras)
  - Morfologia: OPEN -> CLOSE -> mask_clean
  |
  v
[4. EXTRA√á√ÉO DE PARTES - detector._part_boxes()]
  - findContours(mask_clean) -> contours
  - Filtrar por √°rea m√≠nima -> part_boxes
  - Ordenar por y crescente -> part_boxes_sorted
  |
  v
[5. AGRUPAMENTO - detector._build_groups()]
  - Construir grafo de conectividade (overlap horizontal + gap vertical)
  - DFS para componentes conectados -> groups
  |
  v
[6. VALIDA√á√ÉO - detector.detect()]
  Para cada group:
    - Uni√£o de bboxes + padding -> candidate_bbox
    - Validar √°rea: min < area < max
    - Validar aspect: min < h/w < max
    - Validar fill: min < pixels_brancos/area < max
    - Calcular profile_score (forma c√¥nica)
    - Calcular score final: weighted_sum(profile, fill, aspect)
    - Se score >= threshold: ACEITAR
    - Sen√£o: REJEITAR
  |
  - Ordenar aceitos por score -> detections = [(bbox, score, data), ...]
  |
  v
[7. ASSOCIA√á√ÉO - tracker._associate_greedy()]
  - Para cada track existente:
    + Calcular dist√¢ncia euclidiana at√© cada detec√ß√£o
    + Se dist <= max_distance: adicionar a candidatos
  - Ordenar candidatos por dist√¢ncia
  - Associa√ß√£o greedy (primeiro dispon√≠vel) -> matches
  |
  v
[8. ATUALIZA√á√ÉO - tracker.update()]
  - Para tracks associados:
    + Atualizar posi√ß√£o via EMA: pos_new = Œ±*det + (1-Œ±)*pos_old
    + Adicionar score ao hist√≥rico
    + Resetar miss_counter
  - Para tracks n√£o associados:
    + Se CONFIRMED: incrementar miss_counter
    + Se miss_counter > grace_period: voltar a SUSPECT
  - Para detec√ß√µes n√£o associadas:
    + Criar novos tracks (se < max_tracks)
  - Deletar tracks com timeout
  |
  v
[9. CONFIRMA√á√ÉO - tracker.update()]
  Para cada track:
    - frames_vistos = len(score_hist)
    - avg_score = mean(score_hist)
    - Se frames_vistos >= min_frames E avg_score >= threshold:
      + state = CONFIRMED
  |
  -> confirmed_tracks = [tracks com state==CONFIRMED]
  |
  v
[10. C√ÅLCULO DE HEADING - app._debug_print_heading()]
  Para cada confirmed track:
    - err_px = track.cx - frame_center_x
    - focal_px = (frame_w/2) / tan(hfov/2)
    - angle_deg = atan(err_px / focal_px) * 180/œÄ
    - Se cone_height_m conhecido:
      + dist_m = (cone_height_m * focal_px) / track.h
  |
  -> HEADING_DBG logs: id, cx, err_px, err_deg, bbox_h, est_dist, avg_score
  |
  v
[11. VISUALIZA√á√ÉO - visualizer.draw()]
  - Desenhar FPS, contadores, rejects
  - Para cada track (confirmed ou suspect se habilitado):
    + Desenhar bbox colorido
    + Desenhar label: "ID {id} {state} avg={score}"
  |
  -> frame_anotado
  |
  v
[12. SA√çDA]
  - Se video_writer: gravar frame_anotado
  - Se show_windows: exibir frame_anotado em janela
  - Processar teclas (q, s, r)
  |
  v
LOOP para frame n+1

CONTRATOS DE DADOS (TIPOS E FORMATOS):
--------------------------------------

Tipo: Detection
  Formato: Tuple[Tuple[int, int, int, int], float, Dict[str, Any]]
  Estrutura:
    - [0]: bbox = (x: int, y: int, w: int, h: int)
    - [1]: score = float (0.0 a 1.0)
    - [2]: data = {
        "profile": float,        # Profile score (0-1)
        "fill": float,           # Fill ratio (0-1)
        "aspect": float,         # Aspect ratio h/w
        "aspect_score": float,   # Score normalizado de aspect
        "score": float,          # Score final weighted
        "parts": int             # N√∫mero de partes conectadas
      }

Tipo: Track
  Estrutura: Dataclass com campos:
    - track_id: int (√∫nico, crescente)
    - cx, cy: float (centro do bbox)
    - w, h: float (dimens√µes)
    - state: ConeState (SUSPECT, CONFIRMED, LOST)
    - created_at: float (timestamp de cria√ß√£o)
    - last_seen: float (timestamp √∫ltima detec√ß√£o)
    - last_good_time: float (timestamp √∫ltima vez CONFIRMED)
    - score_hist: deque[float] (hist√≥rico de scores, maxlen=10)
    - miss_counter: int (frames consecutivos sem detec√ß√£o)

Tipo: Rejection
  Formato: Tuple[Tuple[int, int, int, int], str]
  Estrutura:
    - [0]: bbox = (x: int, y: int, w: int, h: int)
    - [1]: reason = str (ex: "area=850", "aspect=0.8", "score=0.28")

Tipo: Config
  Formato: Dict[str, Any] (nested dicts)
  Se√ß√µes: camera, debug, hsv_orange, morphology, grouping, geometry, weights, tracking, clahe, color

MENSAGENS INTERNAS (LOGS E DEBUG):
----------------------------------

Log de Detec√ß√£o:
  - "üî¥ Frame com {n} rejei√ß√µes:"
  - "   ‚úó {reason}" (ex: "area=850", "score=0.28")

Log de Tracking:
  - "‚úÖ Track {id} CONFIRMADO! frames={n}, avg={score:.2f}"
  - "üóëÔ∏è Track {id} DELETADO: frames={n}, avg={score:.2f}, idade={age:.2f}s"
  - "üü° Frame com {n} suspects:"
  - "   ? ID {id}: frames={n}, avg={score:.2f}"

Log de Heading:
  - "HEADING_DBG: detected=True id={id} cx={cx:.1f} err_px={err:+.1f} err_deg={angle:+.2f} bbox_h={h} [est_dist={dist:.2f}m] avg_score={score:.2f}"
  - "HEADING_DBG: detected=False" (se sem tracks)

Log de Config:
  - "‚öôÔ∏è Recarregando configura√ß√£o..."
  - "‚úÖ Configura√ß√£o recarregada com sucesso!"
  - "‚öôÔ∏è Config recarregada!" (overlay no v√≠deo por 3s)

PONTOS DE INTEGRA√á√ÉO:
---------------------

1. ENTRADA DE V√çDEO:
   - Interface: cv2.VideoCapture
   - Formatos suportados: MP4, AVI, RTSP streams, USB cameras (V4L2)
   - Configura√ß√£o: camera.video_path ou camera.index

2. SA√çDA DE CONTROLE (PARA NAVEGA√á√ÉO):
   - Interface principal: Logs HEADING_DBG (stdout)
   - Formato: Texto estruturado parse√°vel
   - Dados: track_id, cx, err_px, err_deg, bbox_h, est_dist, avg_score
   - Uso: Sistema de navega√ß√£o pode parsear logs para decis√µes de steering
   - Controle: Habilitado via debug.print_heading (terminal logs)
   
   - Interface visual alternativa: Overlay de heading em v√≠deo
   - Formato: Texto renderizado sobre bboxes CONFIRMED
   - Dados: LEFT/RIGHT/CENTER ¬±X.XX¬∞ por track
   - Uso: Visualiza√ß√£o em tempo real ou grava√ß√£o de v√≠deo anotado
   - Controle: Habilitado via debug.show_heading_overlay (overlay visual)
   
   Nota: debug.print_heading e debug.show_heading_overlay s√£o independentes e
   podem ser habilitados separadamente ou em conjunto.

3. SA√çDA DE V√çDEO:
   - Interface: cv2.VideoWriter
   - Formato: MP4 (codec mp4v)
   - Configura√ß√£o: camera.output_video_path

4. DEBUG CSV EXPORT (PR√â-PROT√ìTIPO):
   - Objetivo: gerar CSVs normalizados por execu√ß√£o para an√°lise antes do prot√≥tipo.
   - Pasta padr√£o: debug.csv_export.csv_dir (ex.: logs/csv)
   - Header CSV: frame_idx,ts_wallclock_ms,ts_source_ms,source,detected,target_id,cx,cy,err_px,err_norm,err_deg,bbox_h,est_dist_m,avg_score,fps
   - Exemplo: 123,1674052345678,1674052345000,video,true,3,480.0,270.0,-120.0,-0.250,-10.12,80,2.53,0.78,29.8
   - Nota: CSV N√ÉO √© protocolo final para ESP32 ‚Äî uso exclusivo pr√©‚Äëprot√≥tipo.
   - Habilitar: debug.csv_export.enabled=true
 
5. INTERFACE GUI:
   - Interface: cv2.imshow, cv2.waitKey
   - Janelas: "Tracker", "Mask" (opcional)
   - Controles: 'q' (quit), 's' (save config), 'r' (reload)
 
6. CONFIGURA√á√ÉO EXTERNA:
   - Interface: Arquivo YAML (cone_config.yaml)
   - Hot-reload: Detecta mudan√ßas via mtime, recarrega automaticamente

LAT√äNCIAS E PERFORMANCE:
------------------------

Pipeline t√≠pico (960x540, CPU Intel i5):
  - Captura: ~1ms
  - Pr√©-processamento: ~5-8ms
  - Segmenta√ß√£o: ~10-15ms (todas as m√°scaras habilitadas)
  - Morfologia: ~3-5ms
  - Contornos + agrupamento: ~2-5ms
  - Valida√ß√£o: ~1-3ms por candidato
  - Tracking: ~1-2ms
  - Visualiza√ß√£o: ~2-3ms
  - Total: ~25-45ms -> 22-40 FPS

Gargalos principais:
  1. Segmenta√ß√£o de cor (especialmente backprojection)
  2. Opera√ß√µes morfol√≥gicas (se muitas itera√ß√µes)
  3. C√°lculo de profile_score (muitos slices)

Otimiza√ß√µes poss√≠veis:
  - Desabilitar m√°scaras n√£o essenciais (Lab, RG, backproj)
  - Reduzir itera√ß√µes de morfologia
  - Reduzir profile_slices
  - Downscale adicional para detec√ß√£o (ex: 640x360)
  - GPU acelera√ß√£o (OpenCV com CUDA)


================================================================================
ALGORITMOS E MODELOS
================================================================================

PIPELINE ATUAL DE DETEC√á√ÉO:
---------------------------

O sistema atual usa DETEC√á√ÉO POR SEGMENTA√á√ÉO DE COR + VALIDA√á√ÉO GEOM√âTRICA.
N√£o utiliza aprendizado de m√°quina (ML) ou redes neurais.

ETAPA 1: SEGMENTA√á√ÉO POR COR
  Algoritmos usados:
    1. HSV Thresholding (sempre ativo):
       - Dois ranges HSV para capturar laranja (wrap-around em Hue)
       - cv2.inRange() em paralelo
       - Combina√ß√£o via OR l√≥gico
    
    2. Lab Color Space (opcional):
       - Thresholding nos canais a e b (cromaticidade)
       - Independente de luminosidade
    
    3. R/G Chromaticity (opcional):
       - Normaliza√ß√£o: r = R/(R+G+B), g = G/(R+G+B)
       - Thresholding em espa√ßo rg
       - Invariante a intensidade
    
    4. Histogram Backprojection (opcional):
       - Histograma 2D Hue-Saturation pr√©-computado
       - cv2.calcBackProject() para probabilidade de cone
       - Thresholding bin√°rio

  Combina√ß√£o: OR l√≥gico de todas as m√°scaras ativas

ETAPA 2: MORFOLOGIA
  Algoritmos:
    1. Abertura (Opening):
       - cv2.morphologyEx(MORPH_OPEN)
       - Remove ru√≠do (pequenas manchas)
       - Kernel retangular 3x3
    
    2. Fechamento (Closing):
       - cv2.morphologyEx(MORPH_CLOSE)
       - Preenche buracos internos
       - Kernel retangular 7x7

ETAPA 3: EXTRA√á√ÉO DE COMPONENTES
  Algoritmo:
    - cv2.findContours(RETR_EXTERNAL, CHAIN_APPROX_SIMPLE)
    - Filtragem por √°rea m√≠nima
    - Extra√ß√£o de bounding boxes via cv2.boundingRect()

ETAPA 4: AGRUPAMENTO DE PARTES
  Algoritmo de grafo de conectividade:
    1. Constru√ß√£o do grafo:
       - N√≥s = bounding boxes de partes
       - Arestas = partes verticalmente alinhadas e pr√≥ximas
       - Crit√©rios de conex√£o:
         + Gap vertical <= max_y_gap (80 pixels)
         + Sobreposi√ß√£o horizontal >= min_x_overlap_ratio (0.20)
         OU centros horizontais pr√≥ximos (<= max_x_center_diff)
    
    2. Busca em componentes conectados:
       - DFS (Depth-First Search) para encontrar componentes conectados
       - Cada componente = grupo de partes que pertencem ao mesmo cone

ETAPA 5: VALIDA√á√ÉO GEOM√âTRICA
  Filtros em cascata (rejei√ß√£o progressiva):
    1. Filtro de √Årea:
       - area = w * h
       - Rejeita se area < 1400 OU area > 450000
    
    2. Filtro de Aspect Ratio:
       - aspect = h / w
       - Rejeita se aspect < 1.0 OU aspect > 6.0
       - (Cones variam de quadrados a muito alongados)
    
    3. Filtro de Fill Ratio:
       - fill = pixels_brancos / area_total
       - Rejeita se fill < 0.08 OU fill > 0.65
       - (Previne detec√ß√µes muito vazias ou muito densas)
    
    4. Filtro de Profile Score:
       - Analisa crescimento de largura de cima para baixo
       - Calcula monotonicity score e top-vs-bottom ratio
       - Rejeita se score final < min_frame_score (0.35)

ETAPA 6: SCORE FINAL
  Score weighted:
    score = 0.50 * profile + 0.35 * fill + 0.15 * aspect_score
  
  Onde:
    - profile ‚àà [0, 1]: qu√£o c√¥nico √© o perfil
    - fill ‚àà [0, 1]: fill ratio normalizado
    - aspect_score = 1 - |aspect - 2.0| / 2.5 ‚àà [0, 1]: proximidade ao aspect ideal

TRACKING: ALGORITMO GREEDY COM EMA
-----------------------------------

ASSOCIA√á√ÉO (Hungarian vs Greedy):
  Algoritmo atual: GREEDY
    - Vantagem: Simples, r√°pido O(n*m log(nm))
    - Desvantagem: N√£o garante associa√ß√£o global √≥tima
    - Adequado para: <= 10 cones simult√¢neos, baixa densidade
  
  Alternativa: HUNGARIAN (not implemented)
    - Vantagem: Associa√ß√£o globalmente √≥tima O(n¬≥)
    - Desvantagem: Mais complexo, mais lento
    - Adequado para: > 10 cones, alta densidade, oclus√µes frequentes

SUAVIZA√á√ÉO: EXPONENTIAL MOVING AVERAGE (EMA)
  F√≥rmula: pos_new = Œ± * det_pos + (1-Œ±) * pos_old
  Onde:
    - Œ± = ema_alpha (default 0.25)
    - Œ± alto (0.4-0.6): mais reativo, menos suave
    - Œ± baixo (0.1-0.2): mais suave, menos reativo
  
  Benef√≠cios:
    - Reduz jitter de detec√ß√µes ruidosas
    - Suaviza trajet√≥rias
    - Simples e eficiente

GERENCIAMENTO DE ESTADOS:
  M√°quina de estados:
    SUSPECT -> CONFIRMED -> [DELETED]
           ‚Üë‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚îò (grace period)
  
  Transi√ß√µes:
    - SUSPECT -> CONFIRMED: frames >= min_frames E avg_score >= threshold
    - CONFIRMED -> SUSPECT: miss_counter > grace_period
    - * -> DELETED: timeout sem detec√ß√µes associadas

ONDE INSERIR MACHINE LEARNING (SUGEST√ïES):
------------------------------------------

1. DETEC√á√ÉO (Substituir segmenta√ß√£o de cor):
   M√©todo: Object Detection CNN (ex: YOLOv8, Faster R-CNN)
   Benef√≠cios:
     - Robusto a varia√ß√µes de ilumina√ß√£o e cor
     - Detecta cones parcialmente oclu√≠dos
     - Aprende features discriminativas
   Implementa√ß√£o sugerida:
     - Coletar dataset de cones anotados (bbox + classe)
     - Treinar YOLOv8-nano (r√°pido, eficiente)
     - Substituir detector.detect() por infer√™ncia do modelo
     - Manter tracker inalterado

2. CLASSIFICA√á√ÉO (Refinar detec√ß√µes):
   M√©todo: CNN de classifica√ß√£o bin√°ria (cone vs n√£o-cone)
   Benef√≠cios:
     - Reduzir falsos positivos
     - Valida√ß√£o adicional ap√≥s detector
   Implementa√ß√£o sugerida:
     - Extrair crops de detec√ß√µes candidatas
     - Passar por rede classificadora (MobileNetV2, EfficientNet-Lite)
     - Filtrar detec√ß√µes com baixa probabilidade

3. TRACKING (Substituir greedy + EMA):
   M√©todo: DeepSORT (Deep Learning + SORT)
   Benef√≠cios:
     - Re-identifica√ß√£o de cones ap√≥s oclus√£o
     - Features visuais profundas para associa√ß√£o
     - Melhor em ambientes densos
   Implementa√ß√£o sugerida:
     - Usar detector atual ou CNN
     - Adicionar rede de extra√ß√£o de features (ResNet, MobileNet)
     - Implementar DeepSORT tracker

4. PREDI√á√ÉO DE TRAJET√ìRIA:
   M√©todo: LSTM ou Transformer para predi√ß√£o de movimento
   Benef√≠cios:
     - Prever posi√ß√£o futura de cones
     - Melhorar associa√ß√£o em oclus√µes
     - Detectar anomalias (cones movendo inesperadamente)

ALGORITMOS COMPLEMENTARES (N√ÉO IMPLEMENTADOS):
----------------------------------------------

1. KALMAN FILTER:
   - Predi√ß√£o de estado (posi√ß√£o, velocidade)
   - Corre√ß√£o baseada em medi√ß√µes
   - √ötil para rastreamento sob oclus√£o

2. HUNGARIAN ALGORITHM:
   - Associa√ß√£o √≥tima global
   - scipy.optimize.linear_sum_assignment
   - Substituir _associate_greedy()

3. NON-MAXIMUM SUPPRESSION (NMS):
   - Eliminar detec√ß√µes duplicadas (overlapping)
   - √ötil se detector gerar m√∫ltiplas detec√ß√µes por cone

4. OPTICAL FLOW:
   - Rastreamento baseado em movimento de pixels
   - Complementar ao tracking por detec√ß√£o

================================================================================
PIPELINE DE DADOS E TREINO (MACHINE LEARNING)
================================================================================

STATUS ATUAL:
-------------
N√ÉO EXISTE pipeline de treino. Sistema atual √© 100% baseado em regras (handcrafted).

CRIANDO PIPELINE DE TREINO (PROPOSTA):
--------------------------------------

OBJETIVO: Treinar modelo de detec√ß√£o de cones usando YOLOv8

FASE 1: COLETA DE DADOS
  1.1. Captura de v√≠deos/imagens:
       - Gravar v√≠deos em diferentes condi√ß√µes:
         + Ilumina√ß√£o: sol, nublado, sombra, noite
         + Cen√°rios: pista aberta, ambiente urbano, indoor
         + Dist√¢ncias: cones perto (grande) e longe (pequeno)
       - Comando: usar output_video_path para salvar v√≠deos
  
  1.2. Amostragem de frames:
       - Extrair frames em intervalos regulares (ex: 1 a cada 10 frames)
       - Evitar frames muito similares (redund√¢ncia)
       - Comando:
         ```bash
         ffmpeg -i video.mp4 -vf "select=not(mod(n\,10))" -vsync vfr frames/frame_%04d.jpg
         ```
  
  1.3. Diversidade:
       - M√≠nimo 500 imagens
       - Balancear classes (se detectar outros objetos)
       - Incluir casos dif√≠ceis (oclus√£o, ilumina√ß√£o extrema)

FASE 2: ANOTA√á√ÉO
  2.1. Ferramenta de anota√ß√£o:
       - Recomendado: LabelImg, CVAT, Label Studio
       - Formato: YOLO txt ou COCO JSON
  
  2.2. Processo:
       - Desenhar bounding boxes em TODOS os cones vis√≠veis
       - Incluir cones parcialmente vis√≠veis (‚â•30% vis√≠vel)
       - N√£o anotar cones muito pequenos (<10x10 pixels)
  
  2.3. Qualidade:
       - Bbox deve cobrir todo o cone com margem m√≠nima
       - Consist√™ncia entre anotadores (se m√∫ltiplos)
       - Valida√ß√£o cruzada (re-anotar 10% das imagens)

FASE 3: PREPARA√á√ÉO DE DATASET
  3.1. Divis√£o:
       - Treino: 70% (350 imagens)
       - Valida√ß√£o: 20% (100 imagens)
       - Teste: 10% (50 imagens)
  
  3.2. Estrutura de diret√≥rios:
       ```
       dataset/
         ‚îú‚îÄ‚îÄ images/
         ‚îÇ   ‚îú‚îÄ‚îÄ train/
         ‚îÇ   ‚îú‚îÄ‚îÄ val/
         ‚îÇ   ‚îî‚îÄ‚îÄ test/
         ‚îî‚îÄ‚îÄ labels/
             ‚îú‚îÄ‚îÄ train/
             ‚îú‚îÄ‚îÄ val/
             ‚îî‚îÄ‚îÄ test/
       ```
  
  3.3. Arquivo de configura√ß√£o (data.yaml):
       ```yaml
       train: ../dataset/images/train
       val: ../dataset/images/val
       test: ../dataset/images/test
       
       nc: 1  # n√∫mero de classes
       names: ['cone']
       ```

FASE 4: TREINO
  4.1. Instalar YOLOv8:
       ```bash
       pip install ultralytics
       ```
  
  4.2. Script de treino:
       ```python
       from ultralytics import YOLO
       
       # Carregar modelo pr√©-treinado
       model = YOLO('yolov8n.pt')  # nano (mais r√°pido)
       
       # Treinar
       results = model.train(
           data='data.yaml',
           epochs=100,
           imgsz=640,
           batch=16,
           device=0,  # GPU 0 ou 'cpu'
           patience=20,  # early stopping
           save=True,
           project='cone_detection',
           name='yolov8n_cone'
       )
       ```
  
  4.3. Hiperpar√¢metros recomendados:
       - epochs: 100-200 (parar se n√£o melhorar por 20 epochs)
       - imgsz: 640 (ou 416 para mais velocidade)
       - batch: 16 (ajustar conforme mem√≥ria GPU)
       - lr0: 0.01 (learning rate inicial)
       - augment: True (flips, crops, color jitter)

FASE 5: VALIDA√á√ÉO
  5.1. M√©tricas:
       - mAP@0.5: Precis√£o m√©dia @ IoU 0.5
       - mAP@0.5:0.95: Precis√£o m√©dia @ IoU 0.5 a 0.95
       - Precision: TP / (TP + FP)
       - Recall: TP / (TP + FN)
  
  5.2. An√°lise de erros:
       - Visualizar predi√ß√µes em set de valida√ß√£o
       - Identificar padr√µes de falhas (ex: cones pequenos, oclus√£o)
       - Coletar mais dados para casos dif√≠ceis
  
  5.3. Comando de valida√ß√£o:
       ```python
       metrics = model.val(data='data.yaml', split='test')
       print(f"mAP@0.5: {metrics.box.map50}")
       ```

FASE 6: EXPORTA√á√ÉO E INTEGRA√á√ÉO
  6.1. Exportar para formato otimizado:
       ```python
       # ONNX (cross-platform)
       model.export(format='onnx', dynamic=True, simplify=True)
       
       # TensorRT (NVIDIA GPU, mais r√°pido)
       model.export(format='engine', device=0)
       ```
  
  6.2. Integrar no detector.py:
       ```python
       class CNNDetector:
           def __init__(self, model_path):
               from ultralytics import YOLO
               self.model = YOLO(model_path)
           
           def detect(self, frame):
               results = self.model.predict(frame, conf=0.25, iou=0.45)
               detections = []
               for r in results[0].boxes:
                   bbox = r.xyxy[0].cpu().numpy()  # x1,y1,x2,y2
                   conf = float(r.conf)
                   # Converter para (x,y,w,h)
                   x, y, x2, y2 = bbox
                   detections.append(((int(x), int(y), int(x2-x), int(y2-y)), conf, {}))
               return detections, None, []
       ```

REPOSIT√ìRIO ESTRUTURADO (PROPOSTA):
-----------------------------------
```
Trekking-DragonBotZv2/
‚îú‚îÄ‚îÄ cone_tracker/          # C√≥digo atual
‚îú‚îÄ‚îÄ dataset/               # NOVO: Dados de treino
‚îÇ   ‚îú‚îÄ‚îÄ raw_videos/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îú‚îÄ‚îÄ models/                # NOVO: Modelos treinados
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n_cone.pt
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n_cone.onnx
‚îÇ   ‚îî‚îÄ‚îÄ training_logs/
‚îú‚îÄ‚îÄ scripts/               # NOVO: Scripts de treino
‚îÇ   ‚îú‚îÄ‚îÄ train_yolo.py
‚îÇ   ‚îú‚îÄ‚îÄ export_model.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îú‚îÄ‚îÄ tests/                 # Testes existentes
‚îî‚îÄ‚îÄ configs/               # NOVO: Configs de treino
    ‚îî‚îÄ‚îÄ data.yaml
```

REPRODUTIBILIDADE:
-----------------
Para garantir experimentos reproduz√≠veis:
  1. Fixar seeds: random.seed(), np.random.seed(), torch.manual_seed()
  2. Versionar c√≥digo (git tags para cada experimento)
  3. Logar hiperpar√¢metros (MLflow, Weights & Biases)
  4. Salvar checkpoints regularmente
  5. Documentar ambiente (requirements.txt, conda env export)

================================================================================
AVALIA√á√ÉO E M√âTRICAS
================================================================================

COMO MEDIR DESEMPENHO DO SISTEMA ATUAL:
---------------------------------------

M√âTRICA 1: DETEC√á√ÉO (Por Frame)
  - True Positives (TP): Cones detectados corretamente
  - False Positives (FP): N√£o-cones detectados como cones
  - False Negatives (FN): Cones n√£o detectados
  
  M√©tricas derivadas:
    - Precision = TP / (TP + FP)  # Quantas detec√ß√µes s√£o corretas
    - Recall = TP / (TP + FN)     # Quantos cones foram detectados
    - F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

M√âTRICA 2: TRACKING (Por V√≠deo)
  - MOTA (Multiple Object Tracking Accuracy):
    MOTA = 1 - (FN + FP + IDSW) / GT
    Onde: IDSW = ID switches, GT = ground truth objects
  
  - MOTP (Multiple Object Tracking Precision):
    MOTP = Œ£(dist√¢ncias) / TP
    Mede precis√£o de localiza√ß√£o
  
  - IDF1 (ID F1 Score):
    Mede consist√™ncia de IDs ao longo do tempo

M√âTRICA 3: LAT√äNCIA
  - Tempo de processamento por frame (ms)
  - FPS (frames per second)
  - Jitter (varia√ß√£o de lat√™ncia)

M√âTRICA 4: ROBUSTEZ
  - Taxa de sucesso em diferentes condi√ß√µes:
    + Sol direto vs nublado vs sombra
    + Cones perto vs longe
    + Cones est√°ticos vs em movimento
    + C√¢mera parada vs em movimento

SCRIPTS SUGERIDOS PARA AVALIA√á√ÉO:
---------------------------------

Script 1: evaluate_detection.py
```python
#!/usr/bin/env python3
"""
Avalia detec√ß√£o em v√≠deo anotado.
Uso: python evaluate_detection.py --video input.mp4 --annotations annotations.json
"""
import json
import cv2
from cone_tracker import ConeDetector, load_config

def load_annotations(path):
    """Carrega anota√ß√µes no formato COCO JSON."""
    with open(path) as f:
        return json.load(f)

def calculate_iou(box1, box2):
    """Calcula IoU entre dois boxes."""
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2
    
    xi1, yi1 = max(x1, x2), max(y1, y2)
    xi2, yi2 = min(x1+w1, x2+w2), min(y1+h1, y2+h2)
    
    inter_area = max(0, xi2-xi1) * max(0, yi2-yi1)
    box1_area, box2_area = w1*h1, w2*h2
    union_area = box1_area + box2_area - inter_area
    
    return inter_area / (union_area + 1e-6)

def evaluate(video_path, annotations_path, iou_threshold=0.5):
    annotations = load_annotations(annotations_path)
    config = load_config()
    detector = ConeDetector(config)
    
    cap = cv2.VideoCapture(video_path)
    frame_idx = 0
    tp, fp, fn = 0, 0, 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # Detec√ß√µes
        proc = cv2.resize(frame, (960, 540))
        detections, _, _ = detector.detect(proc)
        det_boxes = [d[0] for d in detections]
        
        # Ground truth
        gt_boxes = annotations.get(str(frame_idx), [])
        
        # Matching
        matched_gt = set()
        for det_box in det_boxes:
            best_iou = 0
            best_gt_idx = -1
            for i, gt_box in enumerate(gt_boxes):
                iou = calculate_iou(det_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = i
            
            if best_iou >= iou_threshold and best_gt_idx not in matched_gt:
                tp += 1
                matched_gt.add(best_gt_idx)
            else:
                fp += 1
        
        fn += len(gt_boxes) - len(matched_gt)
        frame_idx += 1
    
    cap.release()
    
    precision = tp / (tp + fp + 1e-6)
    recall = tp / (tp + fn + 1e-6)
    f1 = 2 * precision * recall / (precision + recall + 1e-6)
    
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")
    print(f"TP: {tp}, FP: {fp}, FN: {fn}")
```

Script 2: benchmark_fps.py
```python
#!/usr/bin/env python3
"""
Mede FPS m√©dio do sistema.
Uso: python benchmark_fps.py --video input.mp4 --frames 300
"""
import time
import cv2
from cone_tracker import App

def benchmark(video_path, num_frames=300):
    import cone_tracker.config as cfg
    config = cfg.load_config()
    config['camera']['video_path'] = video_path
    config['debug']['show_windows'] = False
    
    from cone_tracker import ConeDetector, MultiConeTracker
    detector = ConeDetector(config)
    tracker = MultiConeTracker(config)
    
    cap = cv2.VideoCapture(video_path)
    
    times = []
    for i in range(num_frames):
        ret, frame = cap.read()
        if not ret:
            break
        
        proc = cv2.resize(frame, (960, 540))
        
        t0 = time.time()
        detections, _, _ = detector.detect(proc)
        tracker.update(detections)
        t1 = time.time()
        
        times.append(t1 - t0)
    
    cap.release()
    
    import numpy as np
    avg_time = np.mean(times)
    std_time = np.std(times)
    fps = 1.0 / avg_time
    
    print(f"Frames processados: {len(times)}")
    print(f"Tempo m√©dio: {avg_time*1000:.2f} ms")
    print(f"Desvio padr√£o: {std_time*1000:.2f} ms")
    print(f"FPS m√©dio: {fps:.1f}")
    print(f"FPS m√≠nimo: {1.0/max(times):.1f}")
    print(f"FPS m√°ximo: {1.0/min(times):.1f}")
```

COMANDOS DE EXECU√á√ÉO:
--------------------
```bash
# Avaliar detec√ß√£o
python scripts/evaluate_detection.py \
  --video test_videos/pista1.mp4 \
  --annotations test_videos/pista1_annotations.json \
  --iou-threshold 0.5

# Benchmark de performance
python scripts/benchmark_fps.py \
  --video test_videos/pista1.mp4 \
  --frames 300

# Comparar configura√ß√µes
python scripts/compare_configs.py \
  --video test_videos/pista1.mp4 \
  --configs config1.yaml config2.yaml config3.yaml
```


================================================================================
TESTES E VALIDA√á√ÉO
================================================================================

ESTRUTURA DE TESTES EXISTENTE:
------------------------------

O reposit√≥rio possui testes em /tests/:
  - test_debug_features.py: Testes de features de debug (11019 bytes)
  - test_integration.py: Testes de integra√ß√£o (4389 bytes)
  - test_streamlit_formatting.py: Testes de formata√ß√£o (2571 bytes)
  - test_video_input.py: Testes de entrada de v√≠deo (5541 bytes)
  - test_video_output.py: Testes de sa√≠da de v√≠deo (9193 bytes)

TEST SCRIPTS: ESP32 INTEGRATION (tests/exec, tests/esp32)
----------------------------------------------------------

- tests/exec/send_csv.py: envia CSV via serial (115200) e imprime ACKs recebidos.
- tests/esp32/esp32_dev_wroom32/main.ino: sketch Arduino que l√™ CSV e responde ACK/NO_TARGET.
- Como testar manualmente:
  1. Habilitar debug.csv_export.enabled=true e gerar CSV (logs/csv).
  2. Flash no ESP32 (Arduino IDE/CLI ou PlatformIO).
  3. Rodar: python3 tests/exec/send_csv.py --device /dev/ttyUSB0 --csv-file logs/csv/run_*.csv

TESTES UNIT√ÅRIOS RECOMENDADOS (POR SCRIPT):
-------------------------------------------

M√ìDULO: cone_tracker/detector.py
  Testes necess√°rios:
    1. test_preprocess():
       - Verificar que retorna HSV
       - Verificar dimens√µes corretas
       - Verificar aplica√ß√£o de CLAHE no canal V
    
    2. test_get_mask():
       - Verificar combina√ß√£o de m√°scaras via OR
       - Testar cada m√°scara individualmente
       - Verificar morfologia aplicada
    
    3. test_part_boxes():
       - Testar com m√°scara vazia (retorna [])
       - Testar com contornos pequenos (filtrados)
       - Verificar ordena√ß√£o por y
    
    4. test_build_groups():
       - Testar agrupamento de partes verticalmente alinhadas
       - Testar casos de partes isoladas
       - Testar m√∫ltiplos grupos
    
    5. test_profile_score():
       - Verificar score para forma c√¥nica (alto)
       - Verificar score para forma retangular (baixo)
       - Testar casos edge (bbox muito pequeno)
    
    6. test_detect():
       - Testar frame com cones conhecidos
       - Verificar formato de retorno (detections, mask, rejects)
       - Verificar ordena√ß√£o por score

M√ìDULO: cone_tracker/tracker.py
  Testes necess√°rios:
    1. test_track_init():
       - Verificar inicializa√ß√£o de Track
       - Verificar bbox(), avg_score(), frames_seen
    
    2. test_track_update():
       - Verificar suaviza√ß√£o EMA
       - Verificar atualiza√ß√£o de timestamps
       - Verificar adi√ß√£o ao hist√≥rico
    
    3. test_associate_greedy():
       - Testar associa√ß√£o perfeita (1-1)
       - Testar com detec√ß√µes n√£o associadas
       - Testar com tracks n√£o associados
       - Verificar que menor dist√¢ncia √© escolhida
    
    4. test_tracker_update():
       - Testar cria√ß√£o de novos tracks
       - Testar atualiza√ß√£o de tracks existentes
       - Testar dele√ß√£o por timeout
       - Testar transi√ß√£o SUSPECT -> CONFIRMED
       - Testar grace period
    
    5. test_confirmed_tracks():
       - Verificar filtragem por estado CONFIRMED

M√ìDULO: cone_tracker/utils.py
  Testes necess√°rios:
    1. test_clamp():
       - Casos: valor dentro, abaixo, acima do range
    
    2. test_safe_roi():
       - Testar ROI completamente dentro da imagem
       - Testar ROI parcialmente fora
       - Testar ROI completamente fora
    
    3. test_bbox_operations():
       - test_x_overlap_ratio(): v√°rios n√≠veis de overlap
       - test_bbox_union(): m√∫ltiplos boxes
       - test_bbox_center(): c√°lculo correto
       - test_bbox_distance(): dist√¢ncia euclidiana

M√ìDULO: cone_tracker/color_utils.py
  Testes necess√°rios:
    1. test_gray_world():
       - Verificar normaliza√ß√£o de imagem com cast de cor
       - Verificar range de sa√≠da [0, 255]
    
    2. test_rg_chromaticity_mask():
       - Testar com pixel laranja (deve ativar)
       - Testar com pixel azul (n√£o deve ativar)
    
    3. test_backproj():
       - Testar carregamento de histograma
       - Testar gera√ß√£o de m√°scara

M√ìDULO: cone_tracker/config.py
  Testes necess√°rios:
    1. test_deep_merge():
       - Testar merge de dicts nested
       - Verificar preced√™ncia de override
    
    2. test_load_config():
       - Testar com arquivo inexistente (retorna default)
       - Testar com YAML inv√°lido (retorna default)
       - Testar com YAML parcial (merge com default)
    
    3. test_watch_config():
       - Testar detec√ß√£o de mudan√ßas em arquivo

M√ìDULO: cone_tracker/visualizer.py
  Testes necess√°rios:
    1. test_color_rotation():
       - Verificar rota√ß√£o de cores por ID
    
    2. test_draw():
       - Verificar que frame √© modificado
       - Verificar presen√ßa de elementos (FPS, tracks, rejects)

M√ìDULO: cone_tracker/app.py
  Testes necess√°rios:
    1. test_reload_config():
       - Verificar reinicializa√ß√£o de componentes
    
    2. test_debug_print_heading():
       - Verificar c√°lculo de √¢ngulos
       - Verificar estimativa de dist√¢ncia

TESTES DE INTEGRA√á√ÉO RECOMENDADOS:
----------------------------------

1. test_end_to_end_video():
   - Processar v√≠deo completo
   - Verificar que n√£o crashou
   - Verificar n√∫mero de frames processados

2. test_hot_reload():
   - Modificar config durante execu√ß√£o
   - Verificar que recarregou

3. test_headless_mode():
   - Executar com show_windows=false
   - Verificar que salvou v√≠deo de sa√≠da

4. test_multiple_cones():
   - V√≠deo com 3-5 cones
   - Verificar tracking de m√∫ltiplos objetos

EXEMPLO DE TESTE UNIT√ÅRIO:
--------------------------
```python
# tests/test_detector_unit.py
import numpy as np
import pytest
from cone_tracker import ConeDetector, load_config

def test_preprocess_returns_hsv():
    config = load_config()
    detector = ConeDetector(config)
    
    # Criar imagem BGR fake
    bgr = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    # Processar
    hsv = detector.preprocess(bgr)
    
    # Verifica√ß√µes
    assert hsv.shape == bgr.shape
    assert hsv.dtype == np.uint8
    # H deve estar em [0, 180)
    assert hsv[:,:,0].max() < 180

def test_profile_score_cone_shape():
    config = load_config()
    detector = ConeDetector(config)
    
    # Criar m√°scara simulando cone (largo embaixo, estreito em cima)
    mask = np.zeros((100, 100), dtype=np.uint8)
    for y in range(100):
        width = int(10 + (y / 100.0) * 40)  # Cresce de 10 a 50
        x_start = 50 - width // 2
        mask[y, x_start:x_start+width] = 255
    
    bbox = (0, 0, 100, 100)
    score = detector._profile_score(mask, bbox)
    
    # Score deve ser alto (> 0.6) para forma c√¥nica
    assert score > 0.6

def test_profile_score_rectangle_shape():
    config = load_config()
    detector = ConeDetector(config)
    
    # Criar m√°scara retangular (largura constante)
    mask = np.zeros((100, 100), dtype=np.uint8)
    mask[10:90, 30:70] = 255
    
    bbox = (0, 0, 100, 100)
    score = detector._profile_score(mask, bbox)
    
    # Score deve ser baixo (< 0.4) para ret√¢ngulo
    assert score < 0.4
```

EXECUTAR TESTES:
---------------
```bash
# Todos os testes
pytest tests/

# Teste espec√≠fico
pytest tests/test_detector_unit.py::test_profile_score_cone_shape

# Com cobertura
pytest tests/ --cov=cone_tracker --cov-report=html

# Com verbose
pytest tests/ -v
```

================================================================================
IMPLANTA√á√ÉO E INTEGRA√á√ÉO
================================================================================

INTEGRA√á√ÉO COM NAVEGA√á√ÉO/AUTONOMIA:
-----------------------------------

INTERFACE 1: Logs HEADING_DBG (stdout)
  Formato: Texto parse√°vel
  Controle: Habilitado via debug.print_heading=true
  Exemplo:
    "HEADING_DBG: detected=True id=2 cx=520.5 err_px=+40.5 err_deg=+3.12 bbox_h=85 est_dist=2.45m avg_score=0.68"
  
  Parser Python:
    ```python
    import re
    
    def parse_heading_log(line):
        if "detected=False" in line:
            return None
        
        pattern = r"id=(\d+) cx=([\d.]+) err_px=([-+\d.]+) err_deg=([-+\d.]+) bbox_h=(\d+)(?: est_dist=([\d.]+)m)? avg_score=([\d.]+)"
        match = re.search(pattern, line)
        if match:
            return {
                'id': int(match.group(1)),
                'cx': float(match.group(2)),
                'err_px': float(match.group(3)),
                'err_deg': float(match.group(4)),
                'bbox_h': int(match.group(5)),
                'est_dist': float(match.group(6)) if match.group(6) else None,
                'avg_score': float(match.group(7))
            }
        return None
    ```
  
  Uso em navega√ß√£o:
    ```python
    # Subprocess para executar cone tracker
    import subprocess
    
    proc = subprocess.Popen(
        ['python3', 'test7.py'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    for line in proc.stdout:
        if "HEADING_DBG" in line:
            data = parse_heading_log(line)
            if data:
                # Enviar comando de steering
                steering_angle = -data['err_deg']  # Inverter para corre√ß√£o
                send_steering_command(steering_angle)
    ```
  
  NOTA - Overlay Visual Alternativo:
    Al√©m dos logs de terminal (debug.print_heading), o sistema tamb√©m oferece
    overlay visual de heading habilitado via debug.show_heading_overlay=true.
    Quando habilitado, desenha "LEFT/RIGHT/CENTER ¬±X.XX¬∞" acima de cada bbox
    CONFIRMED. √ötil para:
      - Grava√ß√£o de v√≠deo anotado (com camera.output_video_path)
      - Visualiza√ß√£o em tempo real (com debug.show_windows=true)
      - Debug visual sem poluir logs de terminal
    
    Os dois modos (terminal e overlay) s√£o independentes e podem ser usados
    separadamente ou em conjunto. A matem√°tica de c√°lculo de heading √© id√™ntica
    em ambos os casos (ver se√ß√£o "C√ÅLCULO DE HEADING" em visualizer.py).

INTERFACE 2: ROS Node (N√ÉO IMPLEMENTADO - PROPOSTA)
  Criar n√≥ ROS que publica detec√ß√µes:
    ```python
    # cone_tracker_ros_node.py
    import rospy
    from geometry_msgs.msg import PoseArray, Pose
    from cone_tracker import App, load_config
    
    class ConeTrackerNode:
        def __init__(self):
            rospy.init_node('cone_tracker')
            self.pub = rospy.Publisher('/cone_detections', PoseArray, queue_size=10)
            self.config = load_config()
            # Modificar App para callback em vez de loop
        
        def publish_detections(self, tracks):
            msg = PoseArray()
            msg.header.stamp = rospy.Time.now()
            msg.header.frame_id = "camera_link"
            
            for track in tracks:
                pose = Pose()
                # Converter cx, cy para coordenadas mundo (requer calibra√ß√£o)
                pose.position.x = estimate_x(track)
                pose.position.y = estimate_y(track)
                pose.position.z = 0.0
                msg.poses.append(pose)
            
            self.pub.publish(msg)
    ```

INTERFACE 3: API REST (N√ÉO IMPLEMENTADO - PROPOSTA)
  Servidor Flask para consumo remoto:
    ```python
    # cone_tracker_api.py
    from flask import Flask, jsonify, request
    import cv2
    import base64
    from cone_tracker import ConeDetector, MultiConeTracker, load_config
    
    app = Flask(__name__)
    config = load_config()
    detector = ConeDetector(config)
    tracker = MultiConeTracker(config)
    
    @app.route('/detect', methods=['POST'])
    def detect():
        # Receber imagem base64
        img_data = request.json['image']
        img_bytes = base64.b64decode(img_data)
        img_array = np.frombuffer(img_bytes, dtype=np.uint8)
        frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
        
        # Processar
        detections, _, _ = detector.detect(frame)
        tracker.update(detections)
        tracks = tracker.confirmed_tracks()
        
        # Retornar JSON
        result = {
            'tracks': [
                {
                    'id': t.track_id,
                    'bbox': t.bbox(),
                    'score': t.avg_score()
                }
                for t in tracks
            ]
        }
        return jsonify(result)
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=5000)
    ```

REQUISITOS DE LAT√äNCIA:
-----------------------

Aplica√ß√£o em rob√¥ aut√¥nomo:
  - Lat√™ncia aceit√°vel: < 100ms (10 FPS m√≠nimo)
  - Lat√™ncia ideal: < 50ms (20+ FPS)
  - Jitter: < 20ms

Para cumprir requisitos:
  1. Reduzir resolu√ß√£o (640x360 em vez de 960x540)
  2. Desabilitar m√°scaras opcionais (Lab, RG, backproj)
  3. Reduzir profile_slices para 6-8
  4. Usar GPU se dispon√≠vel (CUDA OpenCV)
  5. Otimizar c√≥digo Python (Cython, numba)

OBSERVABILIDADE (PROPOSTA):
---------------------------

Logs estruturados:
  ```python
  import logging
  import json
  
  logger = logging.getLogger(__name__)
  
  def log_tracking_event(event_type, data):
      log_entry = {
          'timestamp': time.time(),
          'event': event_type,
          'data': data
      }
      logger.info(json.dumps(log_entry))
  
  # Uso:
  log_tracking_event('track_confirmed', {'id': 5, 'avg_score': 0.68})
  log_tracking_event('track_deleted', {'id': 3, 'age': 2.5})
  ```

M√©tricas (Prometheus):
  ```python
  from prometheus_client import Counter, Histogram, Gauge
  
  detections_total = Counter('cone_detections_total', 'Total detections')
  tracking_latency = Histogram('cone_tracking_latency_seconds', 'Processing latency')
  active_tracks = Gauge('cone_active_tracks', 'Number of active tracks')
  ```

DEPLOYMENT EM PRODU√á√ÉO:
-----------------------

1. DOCKER:
   ```dockerfile
   # Dockerfile
   FROM python:3.9-slim
   
   # Instalar depend√™ncias do sistema
   RUN apt-get update && apt-get install -y \
       libopencv-dev \
       python3-opencv \
       && rm -rf /var/lib/apt/lists/*
   
   WORKDIR /app
   
   # Instalar depend√™ncias Python
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt
   
   # Copiar c√≥digo
   COPY cone_tracker/ ./cone_tracker/
   COPY test7.py .
   COPY cone_config.yaml .
   
   CMD ["python3", "test7.py"]
   ```
   
   Executar:
   ```bash
   docker build -t cone-tracker .
   docker run -v /dev/video0:/dev/video0 --device /dev/video0 cone-tracker
   ```

2. SYSTEMD SERVICE:
   ```ini
   # /etc/systemd/system/cone-tracker.service
   [Unit]
   Description=Cone Detection and Tracking Service
   After=network.target
   
   [Service]
   Type=simple
   User=robot
   WorkingDirectory=/home/robot/Trekking-DragonBotZv2
   ExecStart=/usr/bin/python3 test7.py
   Restart=always
   RestartSec=5
   
   [Install]
   WantedBy=multi-user.target
   ```
   
   Ativar:
   ```bash
   sudo systemctl enable cone-tracker
   sudo systemctl start cone-tracker
   sudo systemctl status cone-tracker
   ```

3. EDGE DEVICE (Raspberry Pi, Jetson Nano):
   Otimiza√ß√µes:
     - Compilar OpenCV com otimiza√ß√µes ARM/CUDA
     - Usar modelo leve (YOLOv8-nano, MobileNet)
     - Reduzir resolu√ß√£o (640x360 ou menos)
     - Desabilitar visualiza√ß√£o (headless)
     - Overclock se necess√°rio

================================================================================
RISCOS E MITIGA√á√ÉO
================================================================================

RISCOS POR SCRIPT/COMPONENTE:
-----------------------------

DETECTOR (detector.py):
  Risco 1: Falsos positivos em objetos laranjas n√£o-cones
    - Impacto: Alto (navega√ß√£o incorreta)
    - Probabilidade: M√©dia
    - Mitiga√ß√£o:
      + Aumentar min_frame_score para 0.40-0.45
      + Habilitar valida√ß√£o por profile_score rigoroso
      + Adicionar classificador CNN secund√°rio
  
  Risco 2: Falha em condi√ß√µes de baixa ilumina√ß√£o
    - Impacto: Alto (cones n√£o detectados)
    - Probabilidade: Alta
    - Mitiga√ß√£o:
      + Habilitar gray_world e CLAHE
      + Adicionar ilumina√ß√£o IR/LED na c√¢mera
      + Treinar modelo ML robusto a ilumina√ß√£o
  
  Risco 3: Cones parcialmente oclu√≠dos n√£o detectados
    - Impacto: M√©dio
    - Probabilidade: Alta
    - Mitiga√ß√£o:
      + Relaxar min_fill_ratio para 0.05
      + Usar ML (YOLO) que lida melhor com oclus√£o
      + Implementar tracking por predi√ß√£o (Kalman)

TRACKER (tracker.py):
  Risco 1: Tracks deletados prematuramente (lost_timeout curto)
    - Impacto: Alto (perda de consist√™ncia)
    - Probabilidade: Alta com config padr√£o (0.6s)
    - Mitiga√ß√£o:
      + Aumentar lost_timeout para 3.0s m√≠nimo
      + Usar grace_seconds em vez de grace_frames
  
  Risco 2: ID switches frequentes (associa√ß√£o falha)
    - Impacto: M√©dio
    - Probabilidade: M√©dia em ambientes densos
    - Mitiga√ß√£o:
      + Aumentar association_max_distance
      + Implementar Hungarian algorithm
      + Usar DeepSORT com features visuais
  
  Risco 3: Limite de tracks atingido (max_tracks)
    - Impacto: M√©dio (cones n√£o rastreados)
    - Probabilidade: Baixa (default 8 √© razo√°vel)
    - Mitiga√ß√£o:
      + Aumentar max_tracks se necess√°rio
      + Implementar prioriza√ß√£o por score/proximidade

APP (app.py):
  Risco 1: Crash por falha de c√¢mera
    - Impacto: Alto (sistema para)
    - Probabilidade: M√©dia
    - Mitiga√ß√£o:
      + Try-catch em cap.read() j√° implementado
      + Contador de falhas (max_fail) j√° implementado
      + Adicionar auto-restart via systemd
  
  Risco 2: Overflow de disco em grava√ß√£o cont√≠nua
    - Impacto: M√©dio
    - Probabilidade: Alta em uso prolongado
    - Mitiga√ß√£o:
      + Rota√ß√£o de arquivos de v√≠deo (timestamp no nome)
      + Limite de tamanho/idade de arquivos
      + Compress√£o adequada (H.264 em vez de mp4v)
  
  Risco 3: HFOV incorreto (c√°lculo de √¢ngulo errado)
    - Impacto: M√©dio (estimativas incorretas)
    - Probabilidade: M√©dia se n√£o calibrado
    - Mitiga√ß√£o:
      + Calibrar c√¢mera (medir HFOV real)
      + Validar estimativas com medi√ß√µes ground-truth
      + Documentar procedimento de calibra√ß√£o

CONFIG (config.py):
  Risco 1: Config YAML malformado (falha ao parsear)
    - Impacto: Baixo (usa defaults)
    - Probabilidade: Baixa
    - Mitiga√ß√£o:
      + Try-catch j√° implementado
      + Valida√ß√£o de schema (pydantic)
      + Fornecer cone_config.yaml.example v√°lido
  
  Risco 2: Valores inv√°lidos (ex: lost_timeout negativo)
    - Impacto: Alto (comportamento inesperado)
    - Probabilidade: M√©dia
    - Mitiga√ß√£o:
      + Valida√ß√£o de ranges ao carregar
      + Asserts em valores cr√≠ticos
      + Documenta√ß√£o clara de ranges permitidos

GERAL:
  Risco 1: Mem√≥ria insuficiente (leak ou uso excessivo)
    - Impacto: Alto (crash)
    - Probabilidade: Baixa
    - Mitiga√ß√£o:
      + Profiling de mem√≥ria (memory_profiler)
      + Limitar tamanho de hist√≥ricos (deque com maxlen)
      + Monitorar uso de mem√≥ria em produ√ß√£o
  
  Risco 2: CPU/GPU insuficiente (lat√™ncia alta)
    - Impacto: Alto (n√£o atende requisitos tempo real)
    - Probabilidade: M√©dia em hardware limitado
    - Mitiga√ß√£o:
      + Benchmark antes de deployment
      + Otimiza√ß√µes (resolu√ß√£o, m√°scaras, slices)
      + Hardware adequado (Jetson Nano, UP Board)

================================================================================
ROADMAP T√âCNICO - BACKLOG PRIORIZADO
================================================================================

√âPICO 1: ROBUSTEZ E QUALIDADE DE DETEC√á√ÉO
-----------------------------------------

US-1.1: Melhorar detec√ß√£o em baixa ilumina√ß√£o
  Prioridade: ALTA
  Esfor√ßo: M√©dio (2-3 dias)
  Descri√ß√£o: Implementar t√©cnicas de pr√©-processamento avan√ßadas
  Crit√©rios de aceita√ß√£o:
    - Taxa de detec√ß√£o > 80% em v√≠deos com ilumina√ß√£o < 100 lux
    - Falsos positivos < 10% em mesma condi√ß√£o
  Tarefas:
    - [ ] Implementar normaliza√ß√£o Retinex
    - [ ] Ajustar par√¢metros CLAHE (clip_limit, tile_size)
    - [ ] Testar com dataset de baixa ilumina√ß√£o
    - [ ] Validar com m√©tricas (precision, recall)

US-1.2: Reduzir falsos positivos
  Prioridade: ALTA
  Esfor√ßo: M√©dio (2-3 dias)
  Descri√ß√£o: Ajustar thresholds e adicionar valida√ß√µes
  Crit√©rios de aceita√ß√£o:
    - Precision > 90% em v√≠deos de teste
    - Recall mant√©m > 85%
  Tarefas:
    - [ ] Coletar dataset de falsos positivos
    - [ ] An√°lise de caracter√≠sticas (√°rea, aspect, fill, profile)
    - [ ] Ajustar thresholds (min_frame_score, confirm_avg_score)
    - [ ] Implementar valida√ß√£o adicional (contorno complexo, simetria)

US-1.3: Suporte a cones parcialmente oclu√≠dos
  Prioridade: M√âDIA
  Esfor√ßo: Alto (1 semana)
  Descri√ß√£o: Detectar cones com oclus√£o parcial (30-50% vis√≠vel)
  Crit√©rios de aceita√ß√£o:
    - Recall > 70% para cones 30-50% oclu√≠dos
    - N√£o degradar detec√ß√£o de cones totalmente vis√≠veis
  Tarefas:
    - [ ] Relaxar fill_ratio m√≠nimo
    - [ ] Ajustar algoritmo de agrupamento
    - [ ] Ou: treinar modelo YOLO (prefer√≠vel)
    - [ ] Validar com dataset de oclus√µes

√âPICO 2: TRACKING E ESTABILIDADE
--------------------------------

US-2.1: Corrigir timeout prematuro de tracks
  Prioridade: CR√çTICA
  Esfor√ßo: Baixo (1 dia)
  Descri√ß√£o: Aumentar lost_timeout para evitar dele√ß√£o prematura
  Crit√©rios de aceita√ß√£o:
    - Tracks sobrevivem >= 90 frames sem detec√ß√£o (3s @ 30fps)
    - Taxa de confirma√ß√£o > 80% de tracks v√°lidos
  Tarefas:
    - [ ] Atualizar DEFAULT_CONFIG lost_timeout: 3.0
    - [ ] Atualizar cone_config.yaml.example  
    - [ ] Testar com v√≠deos problem√°ticos
    - [ ] Validar taxa de confirma√ß√£o

US-2.2: Implementar predi√ß√£o de movimento (Kalman Filter)
  Prioridade: M√âDIA
  Esfor√ßo: Alto (1 semana)
  Descri√ß√£o: Adicionar Kalman Filter para predizer posi√ß√£o em oclus√µes
  Crit√©rios de aceita√ß√£o:
    - Associa√ß√£o correta > 90% mesmo com gaps de 10 frames
    - N√£o aumentar lat√™ncia > 5ms
  Tarefas:
    - [ ] Implementar classe KalmanFilterTracker
    - [ ] Integrar com Track (substituir EMA simples)
    - [ ] Testar com v√≠deos de oclus√£o
    - [ ] Benchmark de performance

US-2.3: Melhorar associa√ß√£o (Hungarian Algorithm)
  Prioridade: BAIXA
  Esfor√ßo: M√©dio (2-3 dias)
  Descri√ß√£o: Substituir greedy por Hungarian para associa√ß√£o √≥tima
  Crit√©rios de aceita√ß√£o:
    - ID switches reduzidos em 50%
    - MOTA > 85% em v√≠deos de teste
  Tarefas:
    - [ ] Implementar Hungarian via scipy.optimize.linear_sum_assignment
    - [ ] Substituir _associate_greedy()
    - [ ] Comparar m√©tricas (MOTA, IDF1) antes/depois
    - [ ] Benchmark de lat√™ncia

√âPICO 3: MACHINE LEARNING E MODERNIZA√á√ÉO
----------------------------------------

US-3.1: Treinar modelo YOLOv8 para detec√ß√£o
  Prioridade: ALTA
  Esfor√ßo: Alto (2 semanas)
  Descri√ß√£o: Substituir detector baseado em cor por CNN
  Crit√©rios de aceita√ß√£o:
    - mAP@0.5 > 0.85 em dataset de valida√ß√£o
    - Lat√™ncia < 50ms em hardware target (Jetson Nano)
    - Precision > 90%, Recall > 85%
  Tarefas:
    - [ ] Coletar e anotar dataset (500+ imagens)
    - [ ] Dividir train/val/test (70/20/10)
    - [ ] Treinar YOLOv8-nano (100 epochs)
    - [ ] Avaliar m√©tricas e refinar
    - [ ] Exportar para ONNX/TensorRT
    - [ ] Integrar no detector.py

US-3.2: Implementar DeepSORT
  Prioridade: M√âDIA
  Esfor√ßo: Alto (1 semana)
  Descri√ß√£o: Adicionar features visuais profundas para tracking
  Crit√©rios de aceita√ß√£o:
    - Re-identifica√ß√£o correta > 80% ap√≥s oclus√£o
    - IDF1 > 0.80 em v√≠deos de teste
  Tarefas:
    - [ ] Treinar rede de extra√ß√£o de features (ResNet18)
    - [ ] Implementar DeepSORT tracker
    - [ ] Substituir MultiConeTracker
    - [ ] Validar m√©tricas

√âPICO 4: OBSERVABILIDADE E OPS
------------------------------

US-4.1: Logs estruturados e m√©tricas
  Prioridade: M√âDIA
  Esfor√ßo: Baixo (1-2 dias)
  Descri√ß√£o: Adicionar logging JSON e m√©tricas Prometheus
  Crit√©rios de aceita√ß√£o:
    - Logs em formato JSON parse√°vel
    - M√©tricas exportadas em /metrics (Prometheus)
  Tarefas:
    - [ ] Migrar logging para structlog ou python-json-logger
    - [ ] Implementar m√©tricas (prometheus_client)
    - [ ] Dashboard Grafana

US-4.2: Health check e auto-recovery
  Prioridade: M√âDIA
  Esfor√ßo: M√©dio (2 dias)
  Descri√ß√£o: Endpoint de health e restart autom√°tico
  Crit√©rios de aceita√ß√£o:
    - Endpoint /health retorna status (se API REST)
    - Systemd reinicia automaticamente em crash
  Tarefas:
    - [ ] Implementar health check
    - [ ] Configurar systemd service com Restart=always
    - [ ] Testar cen√°rios de falha

US-4.3: Testes automatizados e CI/CD
  Prioridade: ALTA
  Esfor√ßo: M√©dio (3 dias)
  Descri√ß√£o: Pipeline de testes e deployment
  Crit√©rios de aceita√ß√£o:
    - Cobertura de testes > 70%
    - CI roda testes em cada PR
    - CD faz deploy autom√°tico em merge
  Tarefas:
    - [ ] Implementar testes unit√°rios (pytest)
    - [ ] Setup GitHub Actions CI
    - [ ] Setup CD (Docker build + deploy)

√âPICO 5: INTEGRA√á√ïES E INTERFACES
---------------------------------

US-5.1: API REST para integra√ß√£o externa
  Prioridade: M√âDIA
  Esfor√ßo: M√©dio (2-3 dias)
  Descri√ß√£o: Servidor Flask/FastAPI para consumo remoto
  Crit√©rios de aceita√ß√£o:
    - Endpoint POST /detect recebe imagem e retorna tracks
    - Lat√™ncia < 100ms (incluindo rede)
    - Documenta√ß√£o OpenAPI
  Tarefas:
    - [ ] Implementar API com FastAPI
    - [ ] Endpoints: /detect, /health, /config
    - [ ] Documenta√ß√£o Swagger
    - [ ] Testes de carga

US-5.2: N√≥ ROS para integra√ß√£o com stack aut√¥nomo
  Prioridade: ALTA (se usar ROS)
  Esfor√ßo: M√©dio (2-3 dias)
  Descri√ß√£o: Publicar detec√ß√µes como mensagens ROS
  Crit√©rios de aceita√ß√£o:
    - Publica em /cone_detections (PoseArray)
    - Lat√™ncia < 50ms
    - Compat√≠vel com ROS Noetic/Humble
  Tarefas:
    - [ ] Implementar cone_tracker_ros_node.py
    - [ ] Definir mensagem customizada (se necess√°rio)
    - [ ] Integrar com tf2 (transforma√ß√µes)
    - [ ] Documenta√ß√£o de uso

US-5.3: Interface web de monitoramento
  Prioridade: BAIXA
  Esfor√ßo: Alto (1 semana)
  Descri√ß√£o: Dashboard web para visualizar tracking em tempo real
  Crit√©rios de aceita√ß√£o:
    - Stream de v√≠deo com overlays
    - Gr√°ficos de m√©tricas (FPS, tracks, etc.)
    - Controle de par√¢metros (config)
  Tarefas:
    - [ ] Backend WebSocket (Flask-SocketIO)
    - [ ] Frontend (React ou Vue.js)
    - [ ] Streaming de v√≠deo (MJPEG ou WebRTC)
    - [ ] UI de controle de config

√âPICO 6: DOCUMENTA√á√ÉO E USABILIDADE
-----------------------------------

US-6.1: Documenta√ß√£o completa de uso
  Prioridade: ALTA
  Esfor√ßo: M√©dio (2 dias)
  Descri√ß√£o: README detalhado e guias de uso
  Crit√©rios de aceita√ß√£o:
    - Usu√°rio novo consegue rodar sistema em < 30 min
    - Documenta√ß√£o de todos os par√¢metros de config
    - Troubleshooting guide
  Tarefas:
    - [x] README.md completo (j√° existe, revisar)
    - [ ] Guia de calibra√ß√£o de c√¢mera
    - [ ] FAQ e troubleshooting
    - [ ] Exemplos de uso (notebooks)

US-6.2: Ferramentas de tuning interativo
  Prioridade: BAIXA
  Esfor√ßo: Alto (1 semana)
  Descri√ß√£o: GUI para ajustar par√¢metros em tempo real
  Crit√©rios de aceita√ß√£o:
    - Sliders para ajustar HSV ranges, thresholds, etc.
    - Preview em tempo real
    - Salvar config ajustado
  Tarefas:
    - [ ] Implementar GUI (tkinter ou Qt)
    - [ ] Integrar com hot-reload
    - [ ] Salvar snapshots de config

PRIORIZA√á√ÉO GERAL (MVP vs NICE-TO-HAVE):
----------------------------------------

MUST HAVE (MVP):
  1. US-2.1: Corrigir timeout prematuro (CR√çTICO)
  2. US-1.2: Reduzir falsos positivos
  3. US-1.1: Melhorar baixa ilumina√ß√£o
  4. US-4.3: Testes automatizados
  5. US-6.1: Documenta√ß√£o completa

SHOULD HAVE (v2):
  1. US-3.1: Treinar YOLOv8
  2. US-5.2: N√≥ ROS (se aplic√°vel)
  3. US-2.2: Kalman Filter
  4. US-4.1: Logs estruturados

COULD HAVE (v3+):
  1. US-3.2: DeepSORT
  2. US-5.1: API REST
  3. US-2.3: Hungarian Algorithm
  4. US-5.3: Interface web

WON'T HAVE (por agora):
  1. US-6.2: Ferramentas de tuning GUI

================================================================================
FIM DO ROADMAP
================================================================================

Este documento fornece um mapeamento COMPLETO e EXTENSIVAMENTE DETALHADO de
todas as funcionalidades relacionadas √† detec√ß√£o de cones no reposit√≥rio
narutojgdr-sudo/Trekking-DragonBotZv2.

Para atualiza√ß√µes deste roadmap, regenerar com base no c√≥digo atual usando:
  1. An√°lise de mudan√ßas em cone_tracker/*
  2. Revis√£o de novos testes em tests/*
  3. Atualiza√ß√£o de m√©tricas e benchmarks
  4. Incorpora√ß√£o de feedback de produ√ß√£o

√öltima atualiza√ß√£o: 2026-01-15
Vers√£o do roadmap: 1.0
Autor: narutojgdr-sudo (agent-generated)
